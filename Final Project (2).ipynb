{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago Crime Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_memory=\"6000m\"\n",
    "launcher.executor_cores=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_df =spark.read.option(\"header\",\"true\").option(\"delimiter\",\",\").option(\"inferschema\", \"true\").option(\"escape\",\"\\\"\").csv(\"/Chicago.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of Missing or Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{sum, col}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{sum, col}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|_c0| ID|Case Number|Date|Block|IUCR|Primary Type|Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On|Latitude|Longitude|Location|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|  0|  0|          1|   0|    0|   0|           0|          0|                1658|     0|       0|   0|       1|  14|            40|       0|       37083|       37083|   0|         0|   37083|    37083|   37083|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.select(data_df.columns.map(c => sum(col(c).isNull.cast(\"int\")).alias(c)): _*).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_filtered_null_val_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_filtered_null_val_df = data_df.na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|_c0| ID|Case Number|Date|Block|IUCR|Primary Type|Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On|Latitude|Longitude|Location|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|  0|  0|          0|   0|    0|   0|           0|          0|                   0|     0|       0|   0|       0|   0|             0|       0|           0|           0|   0|         0|       0|        0|       0|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_filtered_null_val_df.select(data_filtered_null_val_df.columns.map(c => sum(col(c).isNull.cast(\"int\")).alias(c)): _*).show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of Rows with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 1456714\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of Rows after Filtering Null Values which is 2.5% of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 1418365\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_null_val_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all the boolean values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_without_bool_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_without_bool_df = data_filtered_null_val_df.withColumn(\"Arrest\",when(col(\"Arrest\").equalTo(\"True\"),1).otherwise(when(col(\"Arrest\").equalTo(\"False\"),0))).withColumn(\"Domestic\",when(col(\"Domestic\").equalTo(\"True\"),1).otherwise(when(col(\"Domestic\").equalTo(\"False\"),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DownSampling the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Arrest|  count|\n",
      "+------+-------+\n",
      "|     1| 371057|\n",
      "|     0|1047308|\n",
      "+------+-------+\n",
      "\n",
      "+------+-----+\n",
      "|Arrest|count|\n",
      "+------+-----+\n",
      "|     1|36845|\n",
      "|     0|36848|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "down_sampled_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_bool_df.groupBy(\"Arrest\").count().show(10)\n",
    "val down_sampled_df = data_without_bool_df.stat.sampleBy(\"Arrest\", Map(0 -> 0.03542959664, 1 -> 0.1),111)\n",
    "down_sampled_df.groupBy(\"Arrest\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.attribute.Attribute\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.attribute.Attribute\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCol: Array[String] = Array(_c0, ID, Case Number, Date, Block, IUCR, Primary Type, Description, Location Description, Arrest, Domestic, Beat, District, Ward, Community Area, FBI Code, X Coordinate, Y Coordinate, Year, Updated On, Latitude, Longitude, Location)\n",
       "indexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_0ea7ad4e019e, strIdx_0ec6653b47e4, strIdx_a41e1ecd250c, strIdx_903a6460838f, strIdx_0ad159eafce7, strIdx_8d15ef15a95f, strIdx_b7d5d1e38b8e, strIdx_89b1da305869, strIdx_9037efdf0867, strIdx_8b755fc4a503, strIdx_a2f29f42f7e9, strIdx_35939b224499, strIdx_bf7213ffc4b6, strIdx_1aa76edbffb2, strIdx_5a90af697f2b, strIdx_b8d2c90c52b1, strIdx_a2fde87ddd11, strIdx_962c1c2f5ff2, strIdx_3f6a9564d98b, strIdx_6a3c83c65155, strIdx_89e85751ff4f, strIdx_7ee570908b6e, s..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCol = down_sampled_df.columns\n",
    "   \n",
    "var indexers: Array[StringIndexer] = Array()\n",
    "\n",
    "for (colName <- featureCol)\n",
    "    {\n",
    "      val index = new StringIndexer()\n",
    "        .setInputCol(colName)\n",
    "        .setOutputCol(colName + \"_indexed\")\n",
    "        \n",
    "        indexers = indexers :+ index\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_269098adcbbf\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val pipeline = new Pipeline().setStages(indexers)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_fitted_downsampled: org.apache.spark.ml.PipelineModel = pipeline_269098adcbbf\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "   \n",
    "   val pipeline_fitted_downsampled = pipeline.fit(down_sampled_df)\n",
    "    \n",
    "  //  indexed_downsampled_DF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_44c8349584e9\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler() .setInputCols(Array(\"IUCR_indexed\",\"Primary Type_indexed\",\"Description_indexed\",\"Location Description_indexed\",\"Domestic_indexed\",\"Beat_indexed\",\"District_indexed\",\"Ward_indexed\",\"Community Area_indexed\",\"FBI Code_indexed\")) \n",
    "                .setOutputCol(\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_1: org.apache.spark.ml.Pipeline = pipeline_d426bd64bc50\n",
       "features_DF: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 45 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_1 = new Pipeline().setStages(Array(pipeline_fitted_downsampled,assembler))      \n",
    "\n",
    "val features_DF = pipeline_1.fit(down_sampled_df).transform(down_sampled_df)\n",
    "   // indexedDF2.select(\"features\").show()\n",
    " //new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiSqSelector to get the top 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+--------------+\n",
      "|Features                                         |Arrest_indexed|\n",
      "+-------------------------------------------------+--------------+\n",
      "|[0.0,1.0,1.0,2.0,1.0,112.0,8.0,7.0,24.0,2.0]     |0.0           |\n",
      "|[12.0,10.0,11.0,0.0,0.0,57.0,13.0,4.0,11.0,9.0]  |0.0           |\n",
      "|[7.0,0.0,7.0,6.0,0.0,45.0,8.0,18.0,4.0,0.0]      |0.0           |\n",
      "|[12.0,10.0,11.0,0.0,0.0,172.0,19.0,43.0,31.0,9.0]|0.0           |\n",
      "|[16.0,6.0,15.0,15.0,0.0,211.0,7.0,34.0,20.0,6.0] |0.0           |\n",
      "|[2.0,0.0,3.0,59.0,0.0,22.0,13.0,2.0,11.0,0.0]    |0.0           |\n",
      "|[77.0,7.0,70.0,3.0,0.0,128.0,20.0,41.0,68.0,7.0] |0.0           |\n",
      "|[11.0,8.0,10.0,7.0,0.0,8.0,3.0,9.0,0.0,3.0]      |1.0           |\n",
      "|[11.0,8.0,10.0,4.0,0.0,78.0,8.0,18.0,4.0,3.0]    |1.0           |\n",
      "|[19.0,9.0,19.0,5.0,0.0,72.0,0.0,1.0,0.0,8.0]     |0.0           |\n",
      "|[13.0,0.0,12.0,8.0,0.0,1.0,0.0,3.0,1.0,0.0]      |0.0           |\n",
      "|[13.0,0.0,12.0,2.0,0.0,48.0,8.0,6.0,12.0,0.0]    |0.0           |\n",
      "|[40.0,14.0,39.0,0.0,0.0,198.0,9.0,31.0,17.0,11.0]|1.0           |\n",
      "|[1.0,2.0,2.0,0.0,0.0,193.0,15.0,7.0,34.0,1.0]    |1.0           |\n",
      "|[3.0,1.0,0.0,15.0,0.0,37.0,4.0,6.0,12.0,2.0]     |1.0           |\n",
      "|[2.0,0.0,3.0,6.0,0.0,118.0,13.0,29.0,49.0,0.0]   |0.0           |\n",
      "|[0.0,1.0,1.0,1.0,1.0,85.0,0.0,0.0,16.0,2.0]      |0.0           |\n",
      "|[35.0,1.0,25.0,1.0,0.0,100.0,9.0,31.0,61.0,2.0]  |1.0           |\n",
      "|[7.0,0.0,7.0,8.0,0.0,99.0,20.0,48.0,33.0,0.0]    |0.0           |\n",
      "|[24.0,7.0,27.0,8.0,0.0,99.0,20.0,48.0,33.0,7.0]  |0.0           |\n",
      "+-------------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_DF.select(\"Features\",\"Arrest_indexed\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 5 features selected\n",
      "+------------------------+-------------------------------------------------+\n",
      "|selectedFeatures        |features                                         |\n",
      "+------------------------+-------------------------------------------------+\n",
      "|[0.0,1.0,1.0,2.0,1.0]   |[0.0,1.0,1.0,2.0,1.0,112.0,8.0,7.0,24.0,2.0]     |\n",
      "|[12.0,10.0,11.0,0.0,0.0]|[12.0,10.0,11.0,0.0,0.0,57.0,13.0,4.0,11.0,9.0]  |\n",
      "|[7.0,0.0,7.0,6.0,0.0]   |[7.0,0.0,7.0,6.0,0.0,45.0,8.0,18.0,4.0,0.0]      |\n",
      "|[12.0,10.0,11.0,0.0,0.0]|[12.0,10.0,11.0,0.0,0.0,172.0,19.0,43.0,31.0,9.0]|\n",
      "|[16.0,6.0,15.0,15.0,0.0]|[16.0,6.0,15.0,15.0,0.0,211.0,7.0,34.0,20.0,6.0] |\n",
      "|[2.0,0.0,3.0,59.0,0.0]  |[2.0,0.0,3.0,59.0,0.0,22.0,13.0,2.0,11.0,0.0]    |\n",
      "|[77.0,7.0,70.0,3.0,0.0] |[77.0,7.0,70.0,3.0,0.0,128.0,20.0,41.0,68.0,7.0] |\n",
      "|[11.0,8.0,10.0,7.0,0.0] |[11.0,8.0,10.0,7.0,0.0,8.0,3.0,9.0,0.0,3.0]      |\n",
      "|[11.0,8.0,10.0,4.0,0.0] |[11.0,8.0,10.0,4.0,0.0,78.0,8.0,18.0,4.0,3.0]    |\n",
      "|[19.0,9.0,19.0,5.0,0.0] |[19.0,9.0,19.0,5.0,0.0,72.0,0.0,1.0,0.0,8.0]     |\n",
      "|[13.0,0.0,12.0,8.0,0.0] |[13.0,0.0,12.0,8.0,0.0,1.0,0.0,3.0,1.0,0.0]      |\n",
      "|[13.0,0.0,12.0,2.0,0.0] |[13.0,0.0,12.0,2.0,0.0,48.0,8.0,6.0,12.0,0.0]    |\n",
      "|[40.0,14.0,39.0,0.0,0.0]|[40.0,14.0,39.0,0.0,0.0,198.0,9.0,31.0,17.0,11.0]|\n",
      "|[1.0,2.0,2.0,0.0,0.0]   |[1.0,2.0,2.0,0.0,0.0,193.0,15.0,7.0,34.0,1.0]    |\n",
      "|[3.0,1.0,0.0,15.0,0.0]  |[3.0,1.0,0.0,15.0,0.0,37.0,4.0,6.0,12.0,2.0]     |\n",
      "|[2.0,0.0,3.0,6.0,0.0]   |[2.0,0.0,3.0,6.0,0.0,118.0,13.0,29.0,49.0,0.0]   |\n",
      "|[0.0,1.0,1.0,1.0,1.0]   |[0.0,1.0,1.0,1.0,1.0,85.0,0.0,0.0,16.0,2.0]      |\n",
      "|[35.0,1.0,25.0,1.0,0.0] |[35.0,1.0,25.0,1.0,0.0,100.0,9.0,31.0,61.0,2.0]  |\n",
      "|[7.0,0.0,7.0,8.0,0.0]   |[7.0,0.0,7.0,8.0,0.0,99.0,20.0,48.0,33.0,0.0]    |\n",
      "|[24.0,7.0,27.0,8.0,0.0] |[24.0,7.0,27.0,8.0,0.0,99.0,20.0,48.0,33.0,7.0]  |\n",
      "+------------------------+-------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.ChiSqSelector\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_f7ce6fc00ca6\n",
       "features_top5_DF: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(5)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"Arrest_indexed\")\n",
    "  .setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "\n",
    "val features_top5_DF = selector.fit(features_DF).transform(features_DF)\n",
    "println(s\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\")\n",
    "features_top5_DF.select(\"selectedFeatures\",\"features\").show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: features_top5_DF.type = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_top5_DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.tuning._\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.tuning._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_5cdc5a560d2d\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setLabelCol(\"Arrest_indexed\").setFeaturesCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 0.0,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 0.5,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 1.0,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 0.0,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 0.5,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 1.0,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 0.0,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 0.5,\n",
       "\tlogreg_5cdc5a560d2d-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_5cdc5a560d2d-elasticNetParam: 1.0,\n",
       "\tlogreg_5cdc5a560d2d-reg..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_420686d3ee88\n",
       "cv_lr: org.apache.spark.ml.tuning.CrossValidator = cv_abcff73be39b\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\").setLabelCol(\"Arrest_indexed\").setMetricName(\"areaUnderROC\")\n",
    "val cv_lr = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_lr: org.apache.spark.ml.Pipeline = pipeline_654297e4d9c6\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_lr = new Pipeline().setStages(Array(cv_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: int, ID: int ... 46 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,testing)=features_top5_DF.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 18:41:54 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2018-12-05 18:41:54 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_lr: org.apache.spark.ml.PipelineModel = pipeline_654297e4d9c6\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_lr = pipeline_lr.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions_lr: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_lr = model_lr.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|Arrest_indexed|    selectedFeatures|\n",
      "+----------+--------------+--------------------+\n",
      "|       1.0|           0.0|[12.0,10.0,11.0,0...|\n",
      "|       1.0|           0.0|[77.0,7.0,70.0,3....|\n",
      "|       0.0|           1.0|[3.0,1.0,0.0,15.0...|\n",
      "|       0.0|           0.0|[0.0,1.0,1.0,1.0,...|\n",
      "|       1.0|           1.0|[35.0,1.0,25.0,1....|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_lr.select(\"prediction\",\"Arrest_indexed\",\"selectedFeatures\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for LR on test data = 0.5341419575121575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AUC_lr: Double = 0.5341419575121575\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val AUC_lr = evaluator.evaluate(predictions_lr)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxBins: Int = 310\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_85bb43fe9bfe\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_85bb43fe9bfe-maxBins: 310,\n",
       "\trfc_85bb43fe9bfe-maxDepth: 2,\n",
       "\trfc_85bb43fe9bfe-numTrees: 5\n",
       "}, {\n",
       "\trfc_85bb43fe9bfe-maxBins: 310,\n",
       "\trfc_85bb43fe9bfe-maxDepth: 2,\n",
       "\trfc_85bb43fe9bfe-numTrees: 20\n",
       "}, {\n",
       "\trfc_85bb43fe9bfe-maxBins: 310,\n",
       "\trfc_85bb43fe9bfe-maxDepth: 5,\n",
       "\trfc_85bb43fe9bfe-numTrees: 5\n",
       "}, {\n",
       "\trfc_85bb43fe9bfe-maxBins: 310,\n",
       "\trfc_85bb43fe9bfe-maxDepth: 5,\n",
       "\trfc_85bb43fe9bfe-numTrees: 20\n",
       "}, {\n",
       "\trfc_85bb43fe9bfe-maxBins: 315,\n",
       "\trfc_85bb43fe9bfe-maxDepth: 2,\n",
       "\trfc_85bb43fe9bfe-numTrees: 5\n",
       "}, {\n",
       "\trfc_85bb43fe9bfe-maxBins: 315,\n",
       "\trfc_85bb43fe9bfe-maxDepth: 2,\n",
       "\trfc_85bb43fe9bfe-numTrees: 20\n",
       "}, {\n",
       "\trfc_85bb43fe9bfe-maxBins: 315,\n",
       "\trfc_85bb43..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxBins = 310\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"Arrest_indexed\").setFeaturesCol(\"selectedFeatures\")\n",
    "\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             //.addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .addGrid(rf.maxBins, Array(310, 315, 315))\n",
    "             .build()\n",
    "\n",
    "//val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline_rf = new Pipeline().setStages(Array(cv_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_rf: org.apache.spark.ml.PipelineModel = pipeline_026f91f9b00b\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel_rf = pipeline_rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for RF on test data = 0.7727193605998389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_rf: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n",
       "AUC_rf: Double = 0.7727193605998389\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_rf = pipelineModel_rf.transform(testing)\n",
    "val AUC_rf = evaluator.evaluate(predictions_rf)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC_rf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|Arrest_indexed|    selectedFeatures|\n",
      "+----------+--------------+--------------------+\n",
      "|       0.0|           0.0|[12.0,10.0,11.0,0...|\n",
      "|       0.0|           0.0|[77.0,7.0,70.0,3....|\n",
      "|       0.0|           1.0|[3.0,1.0,0.0,15.0...|\n",
      "|       0.0|           0.0|[0.0,1.0,1.0,1.0,...|\n",
      "|       1.0|           1.0|[35.0,1.0,25.0,1....|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf.select(\"prediction\",\"Arrest_indexed\",\"selectedFeatures\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_5b983ee1b88a\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//GBT\n",
    "// Create a GBT model.\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"Arrest_indexed\")\n",
    "  .setFeaturesCol(\"selectedFeatures\")\n",
    "  .setMaxIter(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_5b983ee1b88a-maxBins: 310,\n",
       "\tgbtc_5b983ee1b88a-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_5b983ee1b88a-maxBins: 315,\n",
       "\tgbtc_5b983ee1b88a-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_5b983ee1b88a-maxBins: 315,\n",
       "\tgbtc_5b983ee1b88a-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_5b983ee1b88a-maxBins: 310,\n",
       "\tgbtc_5b983ee1b88a-maxDepth: 5\n",
       "}, {\n",
       "\tgbtc_5b983ee1b88a-maxBins: 315,\n",
       "\tgbtc_5b983ee1b88a-maxDepth: 5\n",
       "}, {\n",
       "\tgbtc_5b983ee1b88a-maxBins: 315,\n",
       "\tgbtc_5b983ee1b88a-maxDepth: 5\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_a851986584b3\n",
       "cv_gbt: org.apache.spark.ml.tuning.CrossValidator = cv_f8160a71e535\n",
       "pipeline_gbt: org.apache.spark.ml.Pipeline = pipeline_e7d8a7196887\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))    \n",
    "            .addGrid(gbt.maxBins, Array(310, 315, 315))\n",
    "             .build()\n",
    "//.addGrid(gbt.maxIter, Array(10, 20,100))\n",
    "//.addGrid(gbt.maxBins, Array(310, 315, 315))\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\").setLabelCol(\"Arrest_indexed\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv_gbt = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "\n",
    "val pipeline_gbt = new Pipeline().setStages(Array(cv_gbt))\n",
    "\n",
    "\n",
    "//val Array(training,testing)=housing.randomSplit(Array(0.8,0.2),111)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 18:15:53 WARN  CacheManager:66 - Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res11: features_top5_DF.type = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_top5_DF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_gbt: org.apache.spark.ml.PipelineModel = pipeline_e7d8a7196887\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val pipelineModel_gbt = pipeline_gbt.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|Arrest_indexed|    selectedFeatures|\n",
      "+----------+--------------+--------------------+\n",
      "|       0.0|           0.0|[0.0,1.0,1.0,2.0,...|\n",
      "|       0.0|           0.0|[2.0,0.0,3.0,59.0...|\n",
      "|       0.0|           0.0|[19.0,9.0,19.0,5....|\n",
      "|       0.0|           0.0|[13.0,0.0,12.0,8....|\n",
      "|       1.0|           1.0|[35.0,1.0,25.0,1....|\n",
      "|       1.0|           1.0|[23.0,7.0,26.0,18...|\n",
      "|       0.0|           0.0|[22.0,5.0,24.0,4....|\n",
      "|       1.0|           1.0|[4.0,0.0,4.0,7.0,...|\n",
      "|       1.0|           1.0|[9.0,2.0,8.0,2.0,...|\n",
      "|       1.0|           1.0|[9.0,2.0,8.0,3.0,...|\n",
      "|       1.0|           1.0|[1.0,2.0,2.0,0.0,...|\n",
      "|       0.0|           0.0|[12.0,10.0,11.0,1...|\n",
      "|       0.0|           0.0|[18.0,5.0,18.0,3....|\n",
      "|       0.0|           1.0|[3.0,1.0,0.0,3.0,...|\n",
      "|       0.0|           1.0|[6.0,3.0,6.0,35.0...|\n",
      "|       0.0|           0.0|[41.0,0.0,40.0,40...|\n",
      "|       1.0|           1.0|[30.0,2.0,31.0,1....|\n",
      "|       0.0|           0.0|[8.0,3.0,5.0,0.0,...|\n",
      "|       0.0|           0.0|[5.0,4.0,0.0,0.0,...|\n",
      "|       0.0|           0.0|[76.0,0.0,69.0,0....|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.8206757416359317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_gbt: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n",
       "AUC_gbt: Double = 0.8206757416359317\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val predictions_gbt = pipelineModel_gbt.transform(testing)\n",
    "predictions_gbt.select(\"prediction\",\"Arrest_indexed\",\"selectedFeatures\").show()\n",
    "\n",
    "\n",
    "val AUC_gbt = evaluator.evaluate(predictions_gbt)\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC_gbt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr.createOrReplaceTempView(\"table_lr\")\n",
    "predictions_rf.createOrReplaceTempView(\"table_rf\")\n",
    "predictions_gbt.createOrReplaceTempView(\"table_gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.mllib.evaluation._\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import spark.implicits._\n",
    "import org.apache.spark.mllib.evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|selectedFeatures        |\n",
      "+------------------------+\n",
      "|[12.0,10.0,11.0,0.0,0.0]|\n",
      "|[77.0,7.0,70.0,3.0,0.0] |\n",
      "|[3.0,1.0,0.0,15.0,0.0]  |\n",
      "|[0.0,1.0,1.0,1.0,1.0]   |\n",
      "|[35.0,1.0,25.0,1.0,0.0] |\n",
      "|[12.0,10.0,11.0,0.0,0.0]|\n",
      "|[9.0,2.0,8.0,3.0,0.0]   |\n",
      "|[21.0,2.0,22.0,1.0,0.0] |\n",
      "|[33.0,2.0,34.0,3.0,0.0] |\n",
      "|[21.0,2.0,22.0,5.0,0.0] |\n",
      "|[41.0,0.0,40.0,40.0,0.0]|\n",
      "|[7.0,0.0,7.0,10.0,0.0]  |\n",
      "|[3.0,1.0,0.0,16.0,0.0]  |\n",
      "|[4.0,0.0,4.0,4.0,0.0]   |\n",
      "|[30.0,2.0,31.0,1.0,0.0] |\n",
      "|[39.0,2.0,38.0,11.0,0.0]|\n",
      "|[0.0,1.0,1.0,2.0,1.0]   |\n",
      "|[0.0,1.0,1.0,2.0,1.0]   |\n",
      "|[13.0,0.0,12.0,23.0,0.0]|\n",
      "|[27.0,5.0,29.0,2.0,1.0] |\n",
      "+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select selectedFeatures from table_lr\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joins: org.apache.spark.sql.DataFrame = [Arrest_indexed: double, prediction_lr: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joins = spark.sql(\"select l.Arrest_indexed,l.prediction as prediction_lr,r.prediction as prediction_rf,g.prediction as prediction_gbt from table_lr l,table_rf r,table_gbt g where l.selectedFeatures = r.selectedFeatures and l.selectedFeatures = g.selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ensemble: org.apache.spark.sql.DataFrame = [prediction_ensemble: double, Arrest: double]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joins.createOrReplaceTempView(\"join_temp\")\n",
    "val ensemble = spark.sql(\"select CASE WHEN (prediction_lr = prediction_rf OR prediction_lr = prediction_gbt) Then prediction_lr else case when prediction_rf=prediction_gbt then prediction_rf else prediction_lr END  END AS prediction_ensemble,Arrest_indexed as Arrest from  join_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for Ensemble on test data = 0.8275903522365249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[8705] at map at <console>:102\n",
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6fc28a4a\n",
       "AUC_EN: Double = 0.8275903522365249\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsAndLabels=ensemble.selectExpr(\"cast(prediction_ensemble as Double) prediction_ensemble\", \"cast(Arrest as Double) Arrest\").rdd.map(row =>(row.getAs[Double](\"prediction_ensemble\"),row.getAs[Double](\"Arrest\")))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@433a4108\n",
       "AUC_EN: Double = 0.8275903522365249\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new BinaryClassificationMetrics(predictionsAndLabels)\n",
    "val AUC_EN = metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for LR on test data = 0.5341419575121575\n",
      "Area under ROC curve(AUC) for RF on test data = 0.7727193605998389\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.8206757416359317\n",
      "Area under ROC curve(AUC) for Ensemble on test data = 0.8275903522365249\n"
     ]
    }
   ],
   "source": [
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC_lr\")\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC_rf\")\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC_gbt\")\n",
    "println(s\"Area under ROC curve(AUC) for Ensemble on test data = $AUC_EN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
