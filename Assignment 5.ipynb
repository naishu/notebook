{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='2600m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the review.json file  and extract “text” and “stars” attributes\n",
    "\n",
    "find the distribution of “stars”\n",
    "attributes; that is,\n",
    "find the number of reviews for \n",
    "each star value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm18:8088/proxy/application_1543027522566_0001\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = yarn, app id = application_1543027522566_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 20:45:47 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-11-23 20:45:49 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-11-23 20:45:51 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2018-11-23 20:46:00 WARN  Client:66 - Same path resource file:///home/administrator/.ivy2/jars/com.github.master_spark-stemming_2.10-0.2.0.jar added multiple times to distributed cache.\n",
      "+--------------------+-----+\n",
      "|                Text|stars|\n",
      "+--------------------+-----+\n",
      "|The pizza was oka...|    2|\n",
      "|I love this place...|    5|\n",
      "|Terrible. Dry cor...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [Text: string, stars: bigint]\n",
       "res0: reviewsDF.type = [Text: string, stars: bigint]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF=spark.read.json(\"/hadoop-user/review.json\").select(\"Text\",\"stars\")\n",
    "reviewsDF.show(3)\n",
    "reviewsDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|stars|count(stars)|\n",
      "+-----+------------+\n",
      "|    5|         361|\n",
      "|    1|         117|\n",
      "|    3|         173|\n",
      "|    2|          85|\n",
      "|    4|         264|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviewsDF.createOrReplaceTempView(\"reviews\")\n",
    "spark.sql(\"select stars,count(stars) from reviews group by stars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.linalg._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.linalg._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|stars|\n",
      "+-----+\n",
      "|    2|\n",
      "|    5|\n",
      "|    1|\n",
      "|    2|\n",
      "|    5|\n",
      "|    1|\n",
      "|    5|\n",
      "|    5|\n",
      "|    4|\n",
      "|    2|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "splits: Array[Double] = Array(-Infinity, 3.0, Infinity)\n",
       "ratingsData: org.apache.spark.sql.DataFrame = [stars: bigint]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = Array(Double.NegativeInfinity, 3,Double.PositiveInfinity)\n",
    "\n",
    "val  ratingsData= spark.read.json(\"/hadoop-user/review.json\").select(\"stars\")\n",
    "\n",
    "ratingsData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|stars|stars_bucketed|\n",
      "+-----+--------------+\n",
      "|    5|           1.0|\n",
      "|    5|           1.0|\n",
      "|    5|           1.0|\n",
      "|    5|           1.0|\n",
      "|    4|           1.0|\n",
      "|    4|           1.0|\n",
      "|    5|           1.0|\n",
      "|    4|           1.0|\n",
      "|    4|           1.0|\n",
      "|    3|           0.0|\n",
      "|    5|           1.0|\n",
      "|    4|           1.0|\n",
      "|    3|           0.0|\n",
      "|    1|           0.0|\n",
      "|    3|           0.0|\n",
      "|    5|           1.0|\n",
      "|    4|           1.0|\n",
      "|    1|           0.0|\n",
      "|    3|           0.0|\n",
      "|    3|           0.0|\n",
      "+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "splits: Array[Double] = Array(-Infinity, 4.0, Infinity)\n",
       "ratingsData: org.apache.spark.sql.DataFrame = [stars: bigint]\n",
       "bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_2468f59185fc\n",
       "bucketedData: org.apache.spark.sql.DataFrame = [stars: bigint, stars_bucketed: double]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = Array(Double.NegativeInfinity, 4,Double.PositiveInfinity)\n",
    "\n",
    "\n",
    "//val  ratingsData= spark.read.json(\"/hadoop-user/review.json\").select(\"stars\")\n",
    "\n",
    "val  ratingsData= spark.read.json(\"/review.json\").select(\"stars\")\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"stars\")\n",
    "  .setOutputCol(\"stars_bucketed\")\n",
    "  .setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(ratingsData)\n",
    "bucketedData.show\n",
    "// Transform original data into its bucket index.\n",
    "//val bucketedData = bucketizer.transform(ratingsDF)\n",
    "//bucketedData.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|stars_bucketed|  count|\n",
      "+--------------+-------+\n",
      "|           0.0|1785005|\n",
      "|           1.0|3476664|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//val bucketedData = bucketizer.transform(ratingsData)\n",
    "//bucketedData.show\n",
    "bucketedData\n",
    "    .groupBy(\"stars_bucketed\")\n",
    "    .count()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|stars_bucketed| count|\n",
      "+--------------+------+\n",
      "|           0.0|178418|\n",
      "|           1.0|178580|\n",
      "+--------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fractionKeyMap: scala.collection.immutable.Map[Double,Double] = Map(0.0 -> 0.1, 1.0 -> 0.0513424)\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  // Create a fraction map where we are only interested:\n",
    "  // - 50% of the rows that have answer_count = 5\n",
    "  // - 10% of the rows that have answer_count = 10\n",
    "  // - 100% of the rows that have answer_count = 20\n",
    "  // Note also that fractions should be in the range [0, 1]\n",
    "//println(test);\n",
    "\n",
    "//1785005/3476664 = 0.513424 \n",
    "\n",
    "//it is returing zero since it is a int, so as a workaround manually putting the decimal value\n",
    "//else we get zero\n",
    "//will convert to double if time permits\n",
    "  val fractionKeyMap = Map(0.0 -> 0.1, 1.0 -> 0.1*0.513424)\n",
    "\n",
    "  // Stratified sample using the fractionKeyMap.\n",
    "  bucketedData\n",
    "    .stat\n",
    "    .sampleBy(\"stars_bucketed\", fractionKeyMap, 111L)\n",
    "    .groupBy(\"stars_bucketed\")\n",
    "    .count()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Extract TFIDF vectors from \n",
    "the review Text. \n",
    "When creating countVectorizer, use \n",
    "setMinDF(100) \n",
    "to only include words in the feature vector  that appear in at \n",
    "least 100 \n",
    "reviews.\n",
    "Make sure that you remove stop words and punctuations and \n",
    "use stemming as explained in the labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: illegal start of simple expression",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: illegal start of simple expression",
      "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]",
      "                  ^",
      ""
     ]
    }
   ],
   "source": [
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "30: error: object Stemmer is not a member of package org.apache.spark.mllib.feature",
     "output_type": "error",
     "traceback": [
      "<console>:30: error: object Stemmer is not a member of package org.apache.spark.mllib.feature",
      "       import org.apache.spark.mllib.feature.Stemmer",
      "              ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "35: error: object Stemmer is not a member of package org.apache.spark.mllib.feature",
     "output_type": "error",
     "traceback": [
      "<console>:35: error: object Stemmer is not a member of package org.apache.spark.mllib.feature",
      "       import org.apache.spark.mllib.feature.Stemmer",
      "              ^",
      "<console>:66: error: object Stemmer is not a member of package org.apache.spark.mllib.feature",
      "       import org.apache.spark.mllib.feature.Stemmer",
      "              ^",
      "<console>:67: error: not found: type Stemmer",
      "       val stemmed_reviews = new Stemmer().setInputCol(\"filtered_words\").setOutputCol(\"stemmed_words\").transform(reviews_filtered)",
      "                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "//val reviewsDF=spark.read.json(\"/review.json\").select(\"Text\")\n",
    "val reviewsDF=spark.read.json(\"/hadoop-user/review.json\").select(\"Text\")\n",
    "\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "\n",
    "\n",
    "reviewsDF.cache()\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "val tokenized = new RegexTokenizer().setMinTokenLength(2).setToLowercase(true).setInputCol(\"Text\").setOutputCol(\"words\").\n",
    "transform(reviewsDF)\n",
    "\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\"\"))\n",
    "}\n",
    "\n",
    "val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "\n",
    "//use the removePuncUDF to remove all punctuations from words\n",
    "val reviewWords= tokenized.withColumn(\"words\",removePuncUDF($\"words\"))\n",
    "\n",
    "//println(\"A sample of the tokenized reviews after removing punctuations\")\n",
    "//reviewWords.show(5, truncate=55)\n",
    "\n",
    "val reviews_filtered=new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered_words\").transform(reviewWords).select(\"Text\",\"filtered_words\")\n",
    "//println(\"reviews after removing stop words\")\n",
    "//reviews_filtered.show(2, truncate=55)\n",
    "\n",
    "\n",
    "//Stemming using a third party package\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "val stemmed_reviews = new Stemmer().setInputCol(\"filtered_words\").setOutputCol(\"stemmed_words\").transform(reviews_filtered)\n",
    "println(\"After stemming\")\n",
    "stemmed_reviews.select(\"Text\",\"stemmed_words\").show(2, truncate=60)\n",
    "\n",
    "val bow = new CountVectorizer().setInputCol(\"stemmed_words\").setOutputCol(\"BOW\").setMinDF(100).fit(stemmed_reviews).transform(stemmed_reviews)\n",
    "bow.show(3)\n",
    "reviewsDF.unpersist()\n",
    "bow.cache()\n",
    "\n",
    "val tfidf = new IDF().setInputCol(\"BOW\").setOutputCol(\"TFIDF features\").fit(bow).transform(bow)\n",
    "println(\"A sample of TFIDF feature vectors\")\n",
    "tfidf.select(\"TFIDF features\").show(5,truncate=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
