{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature Engineering in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndexer\n",
    "Spark StringIndexer converts a categorical column (column of labels or strings) to a numerical column (column of label indices).A numeric index between $[0,number of labels]$ is assigned to each label. The indices are ordered by label frequencies, so the most frequent label is assigned the value 0, the next frequent label is assigned the value 1 and so on. If the input column is numeric, it is first cast to String and is indexed using the string values.\n",
    "In the code segment below we create a categorical column \"color\" with four distinct values (Red, Blue, Green,Purple) and convert it to a numeric column using StringIndexer (the numeric column is appended to the original dataframe). Note that the most frequent label in the column is \"Red\" and hence, it is assigned  the index 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| color|index|\n",
      "+------+-----+\n",
      "|   Red|  0.0|\n",
      "|  Blue|  2.0|\n",
      "| Green|  3.0|\n",
      "|   Red|  0.0|\n",
      "|Purple|  1.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "df: org.apache.spark.sql.DataFrame = [color: string]\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_3946e6c6097e\n",
       "indexed: org.apache.spark.sql.DataFrame = [color: string, index: double]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "val df=Seq((\"Red\"),(\"Blue\"),(\"Green\"),(\"Red\"),(\"Purple\")).toDF(\"color\")\n",
    "val indexer=new StringIndexer().setOutputCol(\"index\").setInputCol(\"color\")\n",
    "val indexed=indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoderEstimator\n",
    "Spark OneHotEncoderEstimator gets a column of indices of a categorical feature (i.e., the output of StringIndexer) and converts each index to a binary vector (its one-hot encoding). \n",
    "\n",
    "In the code segment below we take the column of color indices (produced by StringIndexer) and convert each index to its one-hot-encoding vector. The output is stored in a column of vectors and appended to the original dataframe. Spark has two types of vector: dense and sparse. A dense vector is basically equivalent to an array of values. A sparse vector at the other hand is used when there are only a few non-zero entries exist in the vector and the rest of entries are zero. A sparse vector is in the form of (size,[indices],[values]) where the first element is the vector size, the second element is an array of indices of the non-zero values, and the third element shows the non-zero values. By default spark stores the result of one-hot-encoding as a sparse vector. \n",
    "\n",
    "For instance, the one-hot-encoding vector for color Red with index 0 in the following code segment is (3,[0],[1.0]) this means that the vector is of size 3 where the first element (at index 0) is 1.0 and the rest of the elements are zero. Similarly the encoding (3,[2],[1.0]) for color \"Blue\" means that the vector has three elements where the third element (element at index 2) 1.0 and the rest of the elements are zero. \n",
    "\n",
    "You might wonder why Green is represented as (3,[],[]). By default, OneHotEncoderEstomator() drops the last category in the encoded vector ( as the reference category). This default behavior is needed in models such as linear and logistic regression to ensure the correct degree of freedom. \n",
    "Linear,logistic, and regularized regressions will be discussed later in this module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+\n",
      "| color|index|     codedvec|\n",
      "+------+-----+-------------+\n",
      "|   Red|  0.0|(3,[0],[1.0])|\n",
      "|  Blue|  2.0|(3,[2],[1.0])|\n",
      "| Green|  3.0|    (3,[],[])|\n",
      "|   Red|  0.0|(3,[0],[1.0])|\n",
      "|Purple|  1.0|(3,[1],[1.0])|\n",
      "+------+-----+-------------+\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- index: double (nullable = false)\n",
      " |-- codedvec: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_d433af08f932\n",
       "encoded: org.apache.spark.sql.DataFrame = [color: string, index: double ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder= new OneHotEncoderEstimator().setInputCols(Array(\"index\")).setOutputCols(Array(\"codedvec\"))\n",
    "val encoded=encoder.fit(indexed).transform(indexed)\n",
    "encoded.show()\n",
    "encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## VectorAssembler\n",
    "VectorAssembler is probably used in every pipelin to help concatenate all your features into one big vector you can then pass this vector to a machine learning model. This is usually used in the last step of a machine learning pipeline and is particularly useful when you perform a number of manipulation on each feature using a variety of transformers and need to gather all of these processed features together before feeding it to a machine learning algorithm. For instance, suppose that we the color column above represents the color of diamonds. Then suppose we also have the carat weigh  for each diamond and we want to concatenate the caretweight and color into a vector using VectorAssembler.\n",
    "\n",
    "The code segment below first creates a dataframe with codedvec and caret columns. Then we use VectorAssembler to assemble these columns to a feature vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|     codedvec|caret_weight|\n",
      "+-------------+------------+\n",
      "|(3,[0],[1.0])|        0.25|\n",
      "|(3,[2],[1.0])|         0.5|\n",
      "|    (3,[],[])|        0.25|\n",
      "|(3,[0],[1.0])|         1.0|\n",
      "|(3,[1],[1.0])|         0.5|\n",
      "+-------------+------------+\n",
      "\n",
      "+------------------+\n",
      "|    feature_vector|\n",
      "+------------------+\n",
      "|[1.0,0.0,0.0,0.25]|\n",
      "| [0.0,0.0,1.0,0.5]|\n",
      "|    (4,[3],[0.25])|\n",
      "| [1.0,0.0,0.0,1.0]|\n",
      "| [0.0,1.0,0.0,0.5]|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "caret: org.apache.spark.sql.DataFrame = [caret_weight: double, id: bigint]\n",
       "encoded_with_id: org.apache.spark.sql.DataFrame = [color: string, index: double ... 2 more fields]\n",
       "features: org.apache.spark.sql.DataFrame = [codedvec: vector, caret_weight: double]\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5a8437c9c215\n",
       "feature_vector: Unit = ()\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Spark does not have a function to merge two dataframes with no common column. \n",
    " * So we artifically created an id column for both dataframes using monotonically_increasing_id() function\n",
    " * and then joined the two dataframes on this common column\n",
    " */\n",
    "val caret=Seq((0.25),(0.5),(0.25),(1.0),(0.5)).toDF(\"caret_weight\").withColumn(\"id\", monotonically_increasing_id())\n",
    "val encoded_with_id=encoded.withColumn(\"id\", monotonically_increasing_id())\n",
    "val features=caret.join(encoded_with_id,\"id\").select(\"codedvec\",\"caret_weight\")\n",
    "features.show()\n",
    "\n",
    "//Now let's use vector assembler to assemble these columns to a single feature vector.\n",
    "val assembler=new VectorAssembler().setInputCols(Array(\"codedvec\",\"caret_weight\")).setOutputCol(\"feature_vector\")\n",
    "val feature_vector=assembler.transform(features).select(\"feature_vector\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
