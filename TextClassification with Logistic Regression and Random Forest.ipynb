{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example in Spark\n",
    "This example demonstrates running a simple logistic regression model as well as a Random Forest classification model in spark, tune their hyper parameters, and evaluate the models using cross validation. We will use stumbleupon evergreen dataset from this kaggle competition: https://www.kaggle.com/c/stumbleupon. Unfortunately, the data use agreement does not allow me to share this dataset outside of the competition platform. Therefore, to download this dataset, you will have to create an account on Kaggle, then go to https://www.kaggle.com/c/stumbleupon/data and click on Download all and accept the data use agreement to download the dataset. Once you downloaded the data, copy train.tsv file to your hdfs. This tab-delimited file would be the data we will be working on.\n",
    "\n",
    "\"StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as \"ephemeral\" or \"evergreen\" (source kaggle). \n",
    "\n",
    "The dataset provided by stumbleupon for the kaggle competition has information on over 7.3K webpages including the boilerplate ( A json object which has the title,keywords, and body of a webpage ) as well as some other meta data information on the webpage along with a user-defined label which indicates whether a webpage is evergreen or not.\n",
    "Our goal is to build a logistic regression model as well as a random forest model to predict whether a webpage is evergreen or not.\n",
    "\n",
    "The dataset is not big; however, the program we will have here is scalable and can be run on big data. The goal of this notebook is to learn how to build and train Logistic Regression and Random Forest models in spark, tune their hyper-parameters and evaluate them using cross-validation.\n",
    "\n",
    "As before, let's first configure our spark shell on yarn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='2000m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read train.tsv from HDFS, cache it, print its schema to see what attributes it has, and view a sample of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1541175619604_0006\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = yarn, app id = application_1541175619604_0006)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 10:02:22 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-11-03 10:02:25 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2018-11-03 10:02:30 WARN  Client:66 - Same path resource file:///home/administrator/.ivy2/jars/com.github.master_spark-stemming_2.10-0.2.0.jar added multiple times to distributed cache.\n",
      "2018-11-03 10:03:48 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- urlid: integer (nullable = true)\n",
      " |-- boilerplate: string (nullable = true)\n",
      " |-- alchemy_category: string (nullable = true)\n",
      " |-- alchemy_category_score: string (nullable = true)\n",
      " |-- avglinksize: double (nullable = true)\n",
      " |-- commonlinkratio_1: double (nullable = true)\n",
      " |-- commonlinkratio_2: double (nullable = true)\n",
      " |-- commonlinkratio_3: double (nullable = true)\n",
      " |-- commonlinkratio_4: double (nullable = true)\n",
      " |-- compression_ratio: double (nullable = true)\n",
      " |-- embed_ratio: double (nullable = true)\n",
      " |-- framebased: integer (nullable = true)\n",
      " |-- frameTagRatio: double (nullable = true)\n",
      " |-- hasDomainLink: integer (nullable = true)\n",
      " |-- html_ratio: double (nullable = true)\n",
      " |-- image_ratio: double (nullable = true)\n",
      " |-- is_news: string (nullable = true)\n",
      " |-- lengthyLinkDomain: integer (nullable = true)\n",
      " |-- linkwordscore: integer (nullable = true)\n",
      " |-- news_front_page: string (nullable = true)\n",
      " |-- non_markup_alphanum_characters: integer (nullable = true)\n",
      " |-- numberOfLinks: integer (nullable = true)\n",
      " |-- numwords_in_url: integer (nullable = true)\n",
      " |-- parametrizedLinkRatio: double (nullable = true)\n",
      " |-- spelling_errors_ratio: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "count of records is: 7395+--------------------+-----+--------------------+----------------+----------------------+-----------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------+----------+-------------+-------------+-----------+-----------+-------+-----------------+-------------+---------------+------------------------------+-------------+---------------+---------------------+---------------------+-----+\n",
      "|                 url|urlid|         boilerplate|alchemy_category|alchemy_category_score|avglinksize|commonlinkratio_1|commonlinkratio_2|commonlinkratio_3|commonlinkratio_4|compression_ratio|embed_ratio|framebased|frameTagRatio|hasDomainLink| html_ratio|image_ratio|is_news|lengthyLinkDomain|linkwordscore|news_front_page|non_markup_alphanum_characters|numberOfLinks|numwords_in_url|parametrizedLinkRatio|spelling_errors_ratio|label|\n",
      "+--------------------+-----+--------------------+----------------+----------------------+-----------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------+----------+-------------+-------------+-----------+-----------+-------+-----------------+-------------+---------------+------------------------------+-------------+---------------+---------------------+---------------------+-----+\n",
      "|http://www.bloomb...| 4042|{\"title\":\"IBM See...|        business|              0.789131|2.055555556|      0.676470588|      0.205882353|      0.047058824|      0.023529412|      0.443783175|        0.0|         0|   0.09077381|            0|0.245831182|0.003883495|      1|                1|           24|              0|                          5424|          170|              8|          0.152941176|          0.079129575|    0|\n",
      "|http://www.popsci...| 8471|{\"title\":\"The Ful...|      recreation|              0.574147|3.677966102|       0.50802139|      0.288770053|      0.213903743|      0.144385027|      0.468648998|        0.0|         0|  0.098707403|            0|0.203489628|0.088652482|      1|                1|           40|              0|                          4973|          187|              9|          0.181818182|          0.125448029|    1|\n",
      "|http://www.menshe...| 1164|{\"title\":\"Fruits ...|          health|              0.996526|2.382882883|      0.562015504|      0.321705426|      0.120155039|      0.042635659|      0.525448029|        0.0|         0|  0.072447859|            0| 0.22640177|0.120535714|      1|                1|           55|              0|                          2240|          258|             11|          0.166666667|          0.057613169|    1|\n",
      "+--------------------+-----+--------------------+----------------+----------------------+-----------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------+----------+-------------+-------------+-----------+-----------+-------+-----------------+-------------+---------------+------------------------------+-------------+---------------+---------------------+---------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [url: string, urlid: int ... 25 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df=spark.read.option(\"header\",\"true\").option(\"delimiter\",\"\\t\").option(\"inferschema\", \"true\").option(\"escape\",\"\\\"\").csv(\"/hadoop-user/data/stumbleupon/train.tsv\")\n",
    "\n",
    "df.cache()\n",
    "df.printSchema()\n",
    "print(\"count of records is: \"+df.count)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Transforming Features\n",
    "The only column we will be using as predictor for this lab,is the boilerplate column. The rest of the columns are found to have no correlation with the outcome by the top winners of this competition. Note that a complete data analysis cycle includes visualization and feature selection steps prior to building the model. However, we do not cover feature selection in this course as it needs more statistical background as well as exploratory data analysis and visualization skills which are out of the scope of this class. \n",
    "\n",
    "The boilerplate feature is a JsonObject with three attributes: title, url, and body. We will extract the body attribute and use it as predictor. I filtered out the webpages with empty body and I concatenated the title, body, and the url (which is a set of keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import scala.util.parsing.json.JSON\n",
       "boilerplateDF: org.apache.spark.sql.DataFrame = [body: string, title: string ... 2 more fields]\n",
       "boilerplate: org.apache.spark.sql.DataFrame = [boilerplate: string, label: int]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._  \n",
    "import scala.util.parsing.json.JSON\n",
    "\n",
    "/* get_json_object is a built-in spark sql function which allows us extract attribute from a json column. \n",
    " * The boilerplateDF has four columns: body,title, url, and label( the outcome variable)\n",
    " * for a list of all spark sql functions refer to: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$ \n",
    " */\n",
    "val boilerplateDF = df.select(get_json_object($\"boilerplate\", \"$.body\").alias(\"body\"),\n",
    "                              get_json_object($\"boilerplate\", \"$.title\").alias(\"title\"),\n",
    "                              get_json_object($\"boilerplate\", \"$.url\").alias(\"url\"),\n",
    "                              $\"label\")\n",
    "                                        \n",
    "/* filter the webpages with empty or null body. Then concatenate body,title,and url together with a space in between\n",
    " * concat_ws is a built-in spark sql function which allows to concatenate multiple strings using a given separator\n",
    " */\n",
    "val boilerplate= boilerplateDF.filter(\"trim(body)!='' or trim(body)!=null\").select(concat_ws(\" \",$\"body\" ,$\"title\", $\"url\").alias(\"boilerplate\"), $\"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building TFIDF vectors\n",
    "Now it is time to extract features from the boilerplate text. We tokenize the text, remove punctuations and stop words, do stemming, create bag of words, and finally, compute TFIDF vectors from the boilerplate text. Since we want to add all these stages to a pipeline later, instead of using a sql statement to remove punctuations, we used an \"SQLTransformer\". \n",
    "\n",
    "A SQLTransformer allows writing a sql to manipulate and transform data and then later add this as a stage to a pipeline model. \n",
    "Currently the only SQL syntax supported by SQLTransformer is \"SELECT ... FROM __THIS__ ...\" where \"__THIS__\" represents the underlying table of the input dataset. The select clause specifies the fields, constants, and expressions to display in the output, and can be any select clause that Spark SQL supports. we can also use Spark SQL built-in function and UDFs to operate on these selected columns. Here we use SQL transformer with our removePuncUDF to remove punctuation from boilerplate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_31b27d646572\n",
       "import org.apache.spark.sql.functions.udf\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_b6c539e2b149\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_fb58939e9617\n",
       "import org.apache.spark.mllib.feature.Stemmer\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_82acf48c4fe9\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_344831b604dd\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_ef564f38cc51\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setMinTokenLength(3).setToLowercase(true).setInputCol(\"boilerplate\").setOutputCol(\"boilerplate_words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "//use the removePuncUDF to remove all punctuations from boilerplate_wordss\n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(boilerplate_words) as boilerplate, label from __THIS__ \")\n",
    "\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"boilerplate\").setOutputCol(\"filtered_boilerplate\")\n",
    "\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_boilerplate\").setOutputCol(\"stemmed_boilerplate\")\n",
    "\n",
    "val vectorizer = new CountVectorizer().setInputCol(\"stemmed_boilerplate\").setOutputCol(\"boilerplate_BOW\")\n",
    "\n",
    "val tfidf = new IDF().setInputCol(\"boilerplate_BOW\").setOutputCol(\"boilerplate_TFIDF\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building, Tunning, and Evaluating a Logistic Regression model \n",
    "Now we create a logistic regression model using LogisticRegression class in spark and set its input column to the boilerplate TFIDF vector and its output column to the user defined label indicating whether a website is evergreen or not.We use binaryClassificationEvaluator with AUC (Area under ROC curve) to evaluate our model. A parameter grid is set up to try different values for hyper-parameters including (regParam and elasticNetParam which are lambda and alpha parameters in elastic_net regularization, respectively, and minDocFreq which is the minimum number of different documents a term must appear in to be included in the vocabulary.) and a 5 fold cross validation is used to tune hyper-parameters. \n",
    "Finally, we create a pipeline of all the preprocesisng stages as well as the logistic regression and cross validation stages and fit it to the training data. Then we evaluate the model with the test data. The AUC we get from fitting a logistic regression to the boilerplage attributes is 86.7%\n",
    "Be patient when you run this code. It will take a while for cross validation to complete. You can leave running and come back to it a couple hours later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 14:48:59 WARN  BlockManager:66 - Asked to remove block broadcast_46437, which does not exist\n",
      "2018-11-03 14:53:11 WARN  BlockManager:66 - Asked to remove block broadcast_48936, which does not exist\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|         probability| stemmed_boilerplate|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|    0|       0.0|[0.64249486683892...|[100 , godina haj...|\n",
      "|    0|       0.0|[0.62265139839694...|[cat, ineffici, d...|\n",
      "|    1|       1.0|[0.17175367401133...|[light, cake, soa...|\n",
      "|    0|       0.0|[0.61628265425505...|[memeri, object, ...|\n",
      "|    0|       0.0|[0.63563644784605...|[australian, news...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for LR on test data = 0.867149775276725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_3ea37684d78f\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_3ea37684d78f-elasticNetParam: 0.0,\n",
       "\tidf_ef564f38cc51-minDocFreq: 5,\n",
       "\tlogreg_3ea37684d78f-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_3ea37684d78f-elasticNetParam: 0.5,\n",
       "\tidf_ef564f38cc51-minDocFreq: 5,\n",
       "\tlogreg_3ea37684d78f-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_3ea37684d78f-elasticNetParam: 1.0,\n",
       "\tidf_ef564f38cc51-minDocFreq: 5,\n",
       "\tlogreg_3ea37684d78f-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_3ea37684d78f-elasticNetParam: 0.0,\n",
       "\tidf_ef564f38cc51-minDocFreq: 10,..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "val lr = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"boilerplate_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(5)\n",
    "\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv))\n",
    "\n",
    "val Array(training,testing)=boilerplate.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"label\", \"prediction\", \"probability\", \"stemmed_boilerplate\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building, Tunning, and Evaluating a RandomForest model\n",
    "\n",
    "Now we create a Random Forest model using RandomForestClassifier in spark to predict the label for this dataset. The pipeline is very similar to the pipeline we created for logistic regression, except that the estimater is set to RandomForestClassifier and the hyper-parameters tuned for Random forest are maxDepth ( The maximum depth of each tree in the random forest) and numTrees( The total number of trees in the RandomForest model). Unfortunately, our tiny cluster runs out of memory when I try to run this code segme. Nevertheless, I leave the code segment here for you to see how RandomForest is run on spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_0 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87852_3 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_4 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_4 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_0 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_1 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_1 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_5 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_1 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_3 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_5 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_2 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87852_1 !\n",
      "2018-11-03 15:16:02 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_0 !\n",
      "2018-11-03 15:16:02 ERROR YarnScheduler:70 - Lost executor 1 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:16:02 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 1 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:16:02 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40190.0 (TID 156548, bd-hm, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:16:02 WARN  TaskSetManager:66 - Lost task 3.0 in stage 40190.0 (TID 156550, bd-hm, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:16:03 WARN  TaskSetManager:66 - Lost task 3.1 in stage 40190.0 (TID 156553, bd-s2, executor 3): FetchFailed(null, shuffleId=18816, mapId=-1, reduceId=3, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18816\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867)\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2018-11-03 15:16:19 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40190.0 (TID 156547, bd-hm, executor 6): FetchFailed(BlockManagerId(1, bd-hm, 34698, None), shuffleId=18816, mapId=1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:34698\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:34698\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:34698\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:16:21 WARN  TaskSetManager:66 - Lost task 1.1 in stage 40190.0 (TID 156554, bd-s1, executor 2): FetchFailed(BlockManagerId(1, bd-hm, 34698, None), shuffleId=18816, mapId=1, reduceId=1, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:34698\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:34698\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:34698\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:18:26 WARN  DFSClient:738 - Slow ReadProcessor read fields took 31850ms (threshold=30000ms); ack: seqno: 122550 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.92.132.60:50010,DS-7ec8a3d4-1847-4575-aeec-481b56d85a7f,DISK]]\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_3 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87852_4 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87852_0 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_2 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87852_2 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_2 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87852_5 !\n",
      "2018-11-03 15:21:03 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_3 !\n",
      "2018-11-03 15:21:03 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 6 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:21:03 ERROR YarnScheduler:70 - Lost executor 6 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:21:03 WARN  TaskSetManager:66 - Lost task 3.0 in stage 40234.0 (TID 156737, bd-hm, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:21:06 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:21:06 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_5 !\n",
      "2018-11-03 15:21:06 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_5 !\n",
      "2018-11-03 15:21:06 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_4 !\n",
      "2018-11-03 15:21:06 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:21:06 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_4 !\n",
      "2018-11-03 15:21:06 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 7 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:21:06 ERROR YarnScheduler:70 - Lost executor 7 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:21:06 WARN  TaskSetManager:66 - Lost task 3.1 in stage 40234.0 (TID 156740, bd-hm, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:21:21 WARN  TaskSetManager:66 - Lost task 3.2 in stage 40234.0 (TID 156741, bd-s1, executor 2): FetchFailed(BlockManagerId(6, bd-hm, 55701, None), shuffleId=18828, mapId=2, reduceId=3, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:55701\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:55701\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:55701\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88182_5 !\n",
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_3 !\n",
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_5 !\n",
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_3 !\n",
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_5 !\n",
      "2018-11-03 15:26:00 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_3 !\n",
      "2018-11-03 15:26:00 ERROR YarnScheduler:70 - Lost executor 9 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:26:00 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 9 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:26:00 WARN  TaskSetManager:66 - Lost task 3.0 in stage 40285.0 (TID 156966, bd-hm, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:27:10 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 8 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:27:10 ERROR YarnScheduler:70 - Lost executor 8 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:27:10 WARN  TaskSetManager:66 - Lost task 2.0 in stage 40312.0 (TID 157087, bd-hm, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:27:10 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40312.0 (TID 157085, bd-hm, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:27:10 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_2 !\n",
      "2018-11-03 15:27:10 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_2 !\n",
      "2018-11-03 15:27:10 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_2 !\n",
      "2018-11-03 15:27:10 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_4 !\n",
      "2018-11-03 15:27:10 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:27:10 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_4 !\n",
      "2018-11-03 15:27:10 WARN  TaskSetManager:66 - Lost task 0.1 in stage 40312.0 (TID 157091, bd-s2, executor 5): FetchFailed(null, shuffleId=18852, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18852\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867)\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2018-11-03 15:27:11 WARN  TaskSetManager:66 - Lost task 2.1 in stage 40312.0 (TID 157092, bd-s1, executor 4): FetchFailed(null, shuffleId=18852, mapId=-1, reduceId=2, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18852\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867)\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2018-11-03 15:30:09 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_3 !\n",
      "2018-11-03 15:30:09 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_2 !\n",
      "2018-11-03 15:30:09 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_3 !\n",
      "2018-11-03 15:30:09 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87894_2 !\n",
      "2018-11-03 15:30:09 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_2 !\n",
      "2018-11-03 15:30:09 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_87891_3 !\n",
      "2018-11-03 15:30:09 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 4 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:30:09 ERROR YarnScheduler:70 - Lost executor 4 on bd-s1: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:30:09 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40340.0 (TID 157218, bd-s1, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:30:09 WARN  TaskSetManager:66 - Lost task 3.0 in stage 40340.0 (TID 157220, bd-s1, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:30:09 WARN  TaskSetManager:66 - Lost task 1.1 in stage 40340.0 (TID 157224, bd-hm, executor 10): FetchFailed(null, shuffleId=18861, mapId=-1, reduceId=1, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18861\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867)\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2018-11-03 15:30:31 WARN  TaskSetManager:66 - Lost task 3.1 in stage 40340.0 (TID 157223, bd-s2, executor 5): FetchFailed(BlockManagerId(4, bd-s1, 40118, None), shuffleId=18861, mapId=2, reduceId=3, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:40118\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:40118\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:40118\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:30:47 WARN  DFSClient:738 - Slow ReadProcessor read fields took 31084ms (threshold=30000ms); ack: seqno: 122986 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.92.132.60:50010,DS-7ec8a3d4-1847-4575-aeec-481b56d85a7f,DISK]]\n",
      "2018-11-03 15:32:31 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:32:31 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:32:31 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_4 !\n",
      "2018-11-03 15:32:31 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_5 !\n",
      "2018-11-03 15:32:31 ERROR YarnScheduler:70 - Lost executor 2 on bd-s1: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:32:31 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40368.0 (TID 157349, bd-s1, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:32:31 WARN  TaskSetManager:66 - Lost task 2.0 in stage 40368.0 (TID 157351, bd-s1, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:32:31 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 2 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:32:31 WARN  TaskSetManager:66 - Lost task 0.1 in stage 40368.0 (TID 157356, bd-hm, executor 11): FetchFailed(null, shuffleId=18870, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18870\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867)\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2018-11-03 15:32:46 WARN  TaskSetManager:66 - Lost task 2.1 in stage 40368.0 (TID 157355, bd-hm, executor 10): FetchFailed(BlockManagerId(2, bd-s1, 36661, None), shuffleId=18870, mapId=4, reduceId=2, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:36661\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:36661\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:36661\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:32:47 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40368.0 (TID 157350, bd-s1, executor 12): FetchFailed(BlockManagerId(2, bd-s1, 36661, None), shuffleId=18870, mapId=5, reduceId=1, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:36661\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:36661\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:36661\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_5 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_3 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_3 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_3 !\n",
      "2018-11-03 15:39:48 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 10 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:48 ERROR YarnScheduler:70 - Lost executor 10 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:48 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40456.0 (TID 157717, bd-hm, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:48 WARN  TaskSetManager:66 - Lost task 2.0 in stage 40456.0 (TID 157719, bd-hm, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_4 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_2 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_2 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_2 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_4 !\n",
      "2018-11-03 15:39:48 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_5 !\n",
      "2018-11-03 15:39:49 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 11 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:49 ERROR YarnScheduler:70 - Lost executor 11 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:49 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40456.0 (TID 157718, bd-hm, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:49 WARN  TaskSetManager:66 - Lost task 3.0 in stage 40456.0 (TID 157720, bd-hm, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:39:50 WARN  TaskSetManager:66 - Lost task 1.1 in stage 40456.0 (TID 157726, bd-s2, executor 5): FetchFailed(null, shuffleId=18894, mapId=-1, reduceId=1, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18894\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867)\n",
      "\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2018-11-03 15:40:05 WARN  TaskSetManager:66 - Lost task 3.1 in stage 40456.0 (TID 157725, bd-s1, executor 13): FetchFailed(BlockManagerId(10, bd-hm, 47549, None), shuffleId=18894, mapId=3, reduceId=3, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:47549\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:47549\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:47549\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:40:07 WARN  TaskSetManager:66 - Lost task 2.1 in stage 40456.0 (TID 157723, bd-s1, executor 13): FetchFailed(BlockManagerId(10, bd-hm, 47549, None), shuffleId=18894, mapId=3, reduceId=2, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:47549\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:47549\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:47549\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:40:10 WARN  TaskSetManager:66 - Lost task 0.1 in stage 40456.0 (TID 157724, bd-s2, executor 3): FetchFailed(BlockManagerId(10, bd-hm, 47549, None), shuffleId=18894, mapId=3, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:47549\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:47549\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:47549\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:42:24 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_5 !\n",
      "2018-11-03 15:42:24 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_3 !\n",
      "2018-11-03 15:42:24 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:42:24 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_3 !\n",
      "2018-11-03 15:42:24 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_3 !\n",
      "2018-11-03 15:42:24 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_5 !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:42:24 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 12 for reason Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:24 ERROR YarnScheduler:70 - Lost executor 12 on bd-s1: Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:24 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40484.0 (TID 157855, bd-s1, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:24 WARN  TaskSetManager:66 - Lost task 4.0 in stage 40484.0 (TID 157859, bd-s1, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_1 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_4 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_2 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_2 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_2 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_1 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_1 !\n",
      "2018-11-03 15:42:27 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_4 !\n",
      "2018-11-03 15:42:27 ERROR YarnScheduler:70 - Lost executor 5 on bd-s2: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:27 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40484.0 (TID 157856, bd-s2, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:27 WARN  TaskSetManager:66 - Lost task 5.0 in stage 40484.0 (TID 157860, bd-s2, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:27 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 5 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:42:39 WARN  TaskSetManager:66 - Lost task 4.1 in stage 40484.0 (TID 157861, bd-s2, executor 3): FetchFailed(BlockManagerId(12, bd-s1, 42342, None), shuffleId=18903, mapId=5, reduceId=4, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:42342\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:42:39 WARN  TaskSetManager:66 - Lost task 0.1 in stage 40484.0 (TID 157862, bd-s1, executor 13): FetchFailed(BlockManagerId(12, bd-s1, 42342, None), shuffleId=18903, mapId=5, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:42342\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:42:43 WARN  TaskSetManager:66 - Lost task 1.1 in stage 40484.0 (TID 157864, bd-s1, executor 13): FetchFailed(BlockManagerId(12, bd-s1, 42342, None), shuffleId=18903, mapId=3, reduceId=1, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:42342\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:42:43 WARN  TaskSetManager:66 - Lost task 5.1 in stage 40484.0 (TID 157863, bd-s2, executor 3): FetchFailed(BlockManagerId(12, bd-s1, 42342, None), shuffleId=18903, mapId=3, reduceId=5, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:42342\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:42342\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:47:29 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_0 !\n",
      "2018-11-03 15:47:29 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88344_0 !\n",
      "2018-11-03 15:47:29 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88341_0 !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:47:30 ERROR YarnScheduler:70 - Lost executor 3 on bd-s2: Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:47:30 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40540.0 (TID 158119, bd-s2, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:47:30 WARN  TaskSetManager:66 - Lost task 4.0 in stage 40540.0 (TID 158123, bd-s2, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:47:30 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 3 for reason Container killed by YARN for exceeding memory limits. 2.5 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:47:45 WARN  TaskSetManager:66 - Lost task 4.1 in stage 40540.0 (TID 158125, bd-hm, executor 14): FetchFailed(BlockManagerId(3, bd-s2, 54996, None), shuffleId=18921, mapId=0, reduceId=4, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s2/10.92.132.62:54996\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s2/10.92.132.62:54996\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s2/10.92.132.62:54996\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:47:45 WARN  TaskSetManager:66 - Lost task 0.1 in stage 40540.0 (TID 158126, bd-s2, executor 16): FetchFailed(BlockManagerId(3, bd-s2, 54996, None), shuffleId=18921, mapId=0, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s2/10.92.132.62:54996\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s2/10.92.132.62:54996\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s2/10.92.132.62:54996\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:50:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_0 !\n",
      "2018-11-03 15:50:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_1 !\n",
      "2018-11-03 15:50:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_3 !\n",
      "2018-11-03 15:50:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_0 !\n",
      "2018-11-03 15:50:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_1 !\n",
      "2018-11-03 15:50:37 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 14 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:50:37 ERROR YarnScheduler:70 - Lost executor 14 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:50:37 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40569.0 (TID 158259, bd-hm, executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:50:37 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40569.0 (TID 158261, bd-hm, executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_5 !\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_4 !\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88794_2 !\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_4 !\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_2 !\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_2 !\n",
      "2018-11-03 15:51:49 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88794_4 !\n",
      "2018-11-03 15:51:49 ERROR YarnScheduler:70 - Lost executor 15 on bd-hm: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:51:49 WARN  TaskSetManager:66 - Lost task 2.0 in stage 40590.0 (TID 158345, bd-hm, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:51:49 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 15 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:52:05 WARN  TaskSetManager:66 - Lost task 2.1 in stage 40590.0 (TID 158349, bd-s2, executor 16): FetchFailed(BlockManagerId(15, bd-hm, 45501, None), shuffleId=18936, mapId=2, reduceId=2, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:45501\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:45501\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:45501\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:52:06 WARN  TaskSetManager:66 - Lost task 5.0 in stage 40590.0 (TID 158348, bd-s2, executor 18): FetchFailed(BlockManagerId(15, bd-hm, 45501, None), shuffleId=18936, mapId=4, reduceId=5, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-hm/10.92.132.60:45501\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-hm/10.92.132.60:45501\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-hm/10.92.132.60:45501\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:54:23 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_0 !\n",
      "2018-11-03 15:54:23 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_5 !\n",
      "2018-11-03 15:54:23 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_0 !\n",
      "2018-11-03 15:54:23 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88794_0 !\n",
      "2018-11-03 15:54:23 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 13 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:54:23 ERROR YarnScheduler:70 - Lost executor 13 on bd-s1: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:54:23 WARN  TaskSetManager:66 - Lost task 1.0 in stage 40612.0 (TID 158439, bd-s1, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:54:23 WARN  TaskSetManager:66 - Lost task 5.0 in stage 40612.0 (TID 158443, bd-s1, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:54:45 WARN  TaskSetManager:66 - Lost task 5.1 in stage 40612.0 (TID 158444, bd-s1, executor 17): FetchFailed(BlockManagerId(13, bd-s1, 55191, None), shuffleId=18942, mapId=0, reduceId=5, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:55191\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:55191\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:55191\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:54:47 WARN  TaskSetManager:66 - Lost task 1.1 in stage 40612.0 (TID 158445, bd-s2, executor 18): FetchFailed(BlockManagerId(13, bd-s1, 55191, None), shuffleId=18942, mapId=0, reduceId=1, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s1/10.92.132.61:55191\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s1/10.92.132.61:55191\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s1/10.92.132.61:55191\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-03 15:57:01 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_3 !\n",
      "2018-11-03 15:57:01 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_11_1 !\n",
      "2018-11-03 15:57:01 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88791_1 !\n",
      "2018-11-03 15:57:01 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_88794_1 !\n",
      "2018-11-03 15:57:01 ERROR YarnScheduler:70 - Lost executor 18 on bd-s2: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:57:01 WARN  TaskSetManager:66 - Lost task 0.0 in stage 40634.0 (TID 158534, bd-s2, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:57:01 WARN  TaskSetManager:66 - Lost task 4.0 in stage 40634.0 (TID 158538, bd-s2, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:57:01 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 18 for reason Container killed by YARN for exceeding memory limits. 2.4 GB of 2.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-11-03 15:57:16 WARN  TaskSetManager:66 - Lost task 4.1 in stage 40634.0 (TID 158540, bd-hm, executor 19): FetchFailed(BlockManagerId(18, bd-s2, 50638, None), shuffleId=18948, mapId=3, reduceId=4, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s2/10.92.132.62:50638\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s2/10.92.132.62:50638\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s2/10.92.132.62:50638\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n",
      "2018-11-03 15:57:21 WARN  TaskSetManager:66 - Lost task 0.1 in stage 40634.0 (TID 158541, bd-hm, executor 20): FetchFailed(BlockManagerId(18, bd-s2, 50638, None), shuffleId=18948, mapId=3, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to bd-s2/10.92.132.62:50638\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:454)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:153)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bd-s2/10.92.132.62:50638\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: bd-s2/10.92.132.62:50638\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\t... 2 more\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\t... 11 more\n",
      "\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"boilerplate_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)\n",
    "\n",
    "val pipeline_rf = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv_rf))\n",
    "\n",
    "val Array(training,testing)=boilerplate.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_rf = pipeline_rf.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel_rf.transform(testing)\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
