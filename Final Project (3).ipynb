{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago Crime Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_memory=\"6000m\"\n",
    "launcher.executor_cores=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_df =spark.read.option(\"header\",\"true\").option(\"delimiter\",\",\").option(\"inferschema\", \"true\").option(\"escape\",\"\\\"\").csv(\"/Chicago.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of Missing or Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{sum, col}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{sum, col}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|_c0| ID|Case Number|Date|Block|IUCR|Primary Type|Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On|Latitude|Longitude|Location|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|  0|  0|          1|   0|    0|   0|           0|          0|                1658|     0|       0|   0|       1|  14|            40|       0|       37083|       37083|   0|         0|   37083|    37083|   37083|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.select(data_df.columns.map(c => sum(col(c).isNull.cast(\"int\")).alias(c)): _*).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_filtered_null_val_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_filtered_null_val_df = data_df.na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|_c0| ID|Case Number|Date|Block|IUCR|Primary Type|Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On|Latitude|Longitude|Location|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "|  0|  0|          0|   0|    0|   0|           0|          0|                   0|     0|       0|   0|       0|   0|             0|       0|           0|           0|   0|         0|       0|        0|       0|\n",
      "+---+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_filtered_null_val_df.select(data_filtered_null_val_df.columns.map(c => sum(col(c).isNull.cast(\"int\")).alias(c)): _*).show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of Rows with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 1456714\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of Rows after Filtering Null Values which is 2.5% of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 1418365\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_null_val_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all the boolean values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_without_bool_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_without_bool_df = data_filtered_null_val_df.withColumn(\"Arrest\",when(col(\"Arrest\").equalTo(\"True\"),1).otherwise(when(col(\"Arrest\").equalTo(\"False\"),0))).withColumn(\"Domestic\",when(col(\"Domestic\").equalTo(\"True\"),1).otherwise(when(col(\"Domestic\").equalTo(\"False\"),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DownSampling the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Arrest|  count|\n",
      "+------+-------+\n",
      "|     1| 371057|\n",
      "|     0|1047308|\n",
      "+------+-------+\n",
      "\n",
      "+------+-----+\n",
      "|Arrest|count|\n",
      "+------+-----+\n",
      "|     1|36845|\n",
      "|     0|36848|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "down_sampled_df: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_bool_df.groupBy(\"Arrest\").count().show(10)\n",
    "val down_sampled_df = data_without_bool_df.stat.sampleBy(\"Arrest\", Map(0 -> 0.03542959664, 1 -> 0.1),111)\n",
    "down_sampled_df.groupBy(\"Arrest\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.attribute.Attribute\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.attribute.Attribute\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCol: Array[String] = Array(_c0, ID, Case Number, Date, Block, IUCR, Primary Type, Description, Location Description, Arrest, Domestic, Beat, District, Ward, Community Area, FBI Code, X Coordinate, Y Coordinate, Year, Updated On, Latitude, Longitude, Location)\n",
       "indexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_0ea7ad4e019e, strIdx_0ec6653b47e4, strIdx_a41e1ecd250c, strIdx_903a6460838f, strIdx_0ad159eafce7, strIdx_8d15ef15a95f, strIdx_b7d5d1e38b8e, strIdx_89b1da305869, strIdx_9037efdf0867, strIdx_8b755fc4a503, strIdx_a2f29f42f7e9, strIdx_35939b224499, strIdx_bf7213ffc4b6, strIdx_1aa76edbffb2, strIdx_5a90af697f2b, strIdx_b8d2c90c52b1, strIdx_a2fde87ddd11, strIdx_962c1c2f5ff2, strIdx_3f6a9564d98b, strIdx_6a3c83c65155, strIdx_89e85751ff4f, strIdx_7ee570908b6e, s..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCol = down_sampled_df.columns\n",
    "   \n",
    "var indexers: Array[StringIndexer] = Array()\n",
    "\n",
    "for (colName <- featureCol)\n",
    "    {\n",
    "      val index = new StringIndexer()\n",
    "        .setInputCol(colName)\n",
    "        .setOutputCol(colName + \"_indexed\")\n",
    "        \n",
    "        indexers = indexers :+ index\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_269098adcbbf\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val pipeline = new Pipeline().setStages(indexers)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_fitted_downsampled: org.apache.spark.ml.PipelineModel = pipeline_269098adcbbf\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "   \n",
    "   val pipeline_fitted_downsampled = pipeline.fit(down_sampled_df)\n",
    "    \n",
    "  //  indexed_downsampled_DF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "200: error: value show is not a member of org.apache.spark.ml.PipelineModel",
     "output_type": "error",
     "traceback": [
      "<console>:200: error: value show is not a member of org.apache.spark.ml.PipelineModel",
      "       pipeline_fitted_downsampled.show()",
      "                                   ^",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_44c8349584e9\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler() .setInputCols(Array(\"IUCR_indexed\",\"Primary Type_indexed\",\"Description_indexed\",\"Location Description_indexed\",\"Domestic_indexed\",\"Beat_indexed\",\"District_indexed\",\"Ward_indexed\",\"Community Area_indexed\",\"FBI Code_indexed\")) \n",
    "                .setOutputCol(\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_1: org.apache.spark.ml.Pipeline = pipeline_d426bd64bc50\n",
       "features_DF: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 45 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_1 = new Pipeline().setStages(Array(pipeline_fitted_downsampled,assembler))      \n",
    "\n",
    "val features_DF = pipeline_1.fit(down_sampled_df).transform(down_sampled_df)\n",
    "   // indexedDF2.select(\"features\").show()\n",
    " //new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiSqSelector to get the top 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+--------------+\n",
      "|Features                                         |Arrest_indexed|\n",
      "+-------------------------------------------------+--------------+\n",
      "|[0.0,1.0,1.0,2.0,1.0,112.0,8.0,7.0,24.0,2.0]     |0.0           |\n",
      "|[12.0,10.0,11.0,0.0,0.0,57.0,13.0,4.0,11.0,9.0]  |0.0           |\n",
      "|[7.0,0.0,7.0,6.0,0.0,45.0,8.0,18.0,4.0,0.0]      |0.0           |\n",
      "|[12.0,10.0,11.0,0.0,0.0,172.0,19.0,43.0,31.0,9.0]|0.0           |\n",
      "|[16.0,6.0,15.0,15.0,0.0,211.0,7.0,34.0,20.0,6.0] |0.0           |\n",
      "|[2.0,0.0,3.0,59.0,0.0,22.0,13.0,2.0,11.0,0.0]    |0.0           |\n",
      "|[77.0,7.0,70.0,3.0,0.0,128.0,20.0,41.0,68.0,7.0] |0.0           |\n",
      "|[11.0,8.0,10.0,7.0,0.0,8.0,3.0,9.0,0.0,3.0]      |1.0           |\n",
      "|[11.0,8.0,10.0,4.0,0.0,78.0,8.0,18.0,4.0,3.0]    |1.0           |\n",
      "|[19.0,9.0,19.0,5.0,0.0,72.0,0.0,1.0,0.0,8.0]     |0.0           |\n",
      "|[13.0,0.0,12.0,8.0,0.0,1.0,0.0,3.0,1.0,0.0]      |0.0           |\n",
      "|[13.0,0.0,12.0,2.0,0.0,48.0,8.0,6.0,12.0,0.0]    |0.0           |\n",
      "|[40.0,14.0,39.0,0.0,0.0,198.0,9.0,31.0,17.0,11.0]|1.0           |\n",
      "|[1.0,2.0,2.0,0.0,0.0,193.0,15.0,7.0,34.0,1.0]    |1.0           |\n",
      "|[3.0,1.0,0.0,15.0,0.0,37.0,4.0,6.0,12.0,2.0]     |1.0           |\n",
      "|[2.0,0.0,3.0,6.0,0.0,118.0,13.0,29.0,49.0,0.0]   |0.0           |\n",
      "|[0.0,1.0,1.0,1.0,1.0,85.0,0.0,0.0,16.0,2.0]      |0.0           |\n",
      "|[35.0,1.0,25.0,1.0,0.0,100.0,9.0,31.0,61.0,2.0]  |1.0           |\n",
      "|[7.0,0.0,7.0,8.0,0.0,99.0,20.0,48.0,33.0,0.0]    |0.0           |\n",
      "|[24.0,7.0,27.0,8.0,0.0,99.0,20.0,48.0,33.0,7.0]  |0.0           |\n",
      "+-------------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_DF.select(\"Features\",\"Arrest_indexed\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_DF.createOrReplaceTempView(\"num_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeric_features: org.apache.spark.sql.DataFrame = [IUCR_indexed: double, Primary Type_indexed: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numeric_features= spark.sql(\"select IUCR_indexed,`Primary Type_indexed`,Description_indexed,Location Description_indexed,Domestic_indexed,Beat_indexed,District_indexed,Ward_indexed,`Community Area_indexed`,`FBI Code_indexed` from num_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.ChiSqSelector\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_181b0bd49d74\n",
       "features_top_DF: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 46 more fields]\n",
       "show_feature_name_df: org.apache.spark.ml.feature.ChiSqSelectorModel = chiSqSelector_181b0bd49d74\n"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    ".setSelectorType(\"fpr\")\n",
    ".setFpr(0.01)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"Arrest_indexed\")\n",
    "  .setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "\n",
    "val features_top_DF = selector.fit(features_DF).transform(features_DF)\n",
    "\n",
    "\n",
    "val show_feature_name_df = selector.fit(features_DF)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "//println(s\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\")\n",
    "//features_top5_DF.select(\"selectedFeatures\",\"features\").show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_name: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val feature_name = show_feature_name_df.selectedFeatures\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Features selected by CHi Square selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "feature_name.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: features_top_DF.type = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_top_DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.tuning._\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.tuning._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_34c53c1821ef\n"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setLabelCol(\"Arrest_indexed\").setFeaturesCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 0.0,\n",
       "\tlogreg_34c53c1821ef-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 0.5,\n",
       "\tlogreg_34c53c1821ef-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 1.0,\n",
       "\tlogreg_34c53c1821ef-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 0.0,\n",
       "\tlogreg_34c53c1821ef-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 0.5,\n",
       "\tlogreg_34c53c1821ef-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 1.0,\n",
       "\tlogreg_34c53c1821ef-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 0.0,\n",
       "\tlogreg_34c53c1821ef-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 0.5,\n",
       "\tlogreg_34c53c1821ef-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_34c53c1821ef-elasticNetParam: 1.0,\n",
       "\tlogreg_34c53c1821ef-reg..."
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_91d64ab46afc\n",
       "cv_lr: org.apache.spark.ml.tuning.CrossValidator = cv_834460ed96c0\n"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\").setLabelCol(\"Arrest_indexed\").setMetricName(\"areaUnderROC\")\n",
    "val cv_lr = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_lr: org.apache.spark.ml.Pipeline = pipeline_1b079443272a\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_lr = new Pipeline().setStages(Array(cv_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: int, ID: int ... 46 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,testing)=features_top_DF.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06 15:34:55 WARN  BlockManager:66 - Asked to remove block broadcast_5239, which does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_lr: org.apache.spark.ml.PipelineModel = pipeline_1b079443272a\n"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_lr = pipeline_lr.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions_lr: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_lr = model_lr.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|Arrest_indexed|    selectedFeatures|\n",
      "+----------+--------------+--------------------+\n",
      "|       0.0|           0.0|[77.0,7.0,70.0,3....|\n",
      "|       1.0|           0.0|[19.0,9.0,19.0,5....|\n",
      "|       0.0|           1.0|[1.0,2.0,2.0,0.0,...|\n",
      "|       0.0|           1.0|[35.0,1.0,25.0,1....|\n",
      "|       1.0|           0.0|[77.0,7.0,70.0,2....|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_lr.select(\"prediction\",\"Arrest_indexed\",\"selectedFeatures\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for LR on test data = 0.5958459707451914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AUC_lr: Double = 0.5958459707451914\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val AUC_lr = evaluator.evaluate(predictions_lr)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxBins: Int = 310\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_fcb7910c201c\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_fcb7910c201c-maxBins: 310,\n",
       "\trfc_fcb7910c201c-maxDepth: 2,\n",
       "\trfc_fcb7910c201c-numTrees: 5\n",
       "}, {\n",
       "\trfc_fcb7910c201c-maxBins: 315,\n",
       "\trfc_fcb7910c201c-maxDepth: 2,\n",
       "\trfc_fcb7910c201c-numTrees: 5\n",
       "}, {\n",
       "\trfc_fcb7910c201c-maxBins: 315,\n",
       "\trfc_fcb7910c201c-maxDepth: 2,\n",
       "\trfc_fcb7910c201c-numTrees: 5\n",
       "}, {\n",
       "\trfc_fcb7910c201c-maxBins: 310,\n",
       "\trfc_fcb7910c201c-maxDepth: 2,\n",
       "\trfc_fcb7910c201c-numTrees: 20\n",
       "}, {\n",
       "\trfc_fcb7910c201c-maxBins: 315,\n",
       "\trfc_fcb7910c201c-maxDepth: 2,\n",
       "\trfc_fcb7910c201c-numTrees: 20\n",
       "}, {\n",
       "\trfc_fcb7910c201c-maxBins: 315,\n",
       "\trfc_fcb7910c201c-maxDepth: 2,\n",
       "\trfc_fcb7910c201c-numTrees: 20\n",
       "}, {\n",
       "\trfc_fcb7910c201c-maxBins: 310,\n",
       "\trfc_fcb791..."
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxBins = 310\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"Arrest_indexed\").setFeaturesCol(\"selectedFeatures\")\n",
    "\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             //.addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .addGrid(rf.maxBins, Array(310, 315, 315))\n",
    "             .build()\n",
    "\n",
    "//val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline_rf = new Pipeline().setStages(Array(cv_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_rf: org.apache.spark.ml.PipelineModel = pipeline_b4b9d84e1462\n"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel_rf = pipeline_rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for RF on test data = 0.794775709187095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_rf: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n",
       "AUC_rf: Double = 0.794775709187095\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_rf = pipelineModel_rf.transform(testing)\n",
    "val AUC_rf = evaluator.evaluate(predictions_rf)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC_rf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|Arrest_indexed|    selectedFeatures|\n",
      "+----------+--------------+--------------------+\n",
      "|       0.0|           0.0|[77.0,7.0,70.0,3....|\n",
      "|       0.0|           0.0|[19.0,9.0,19.0,5....|\n",
      "|       1.0|           1.0|[1.0,2.0,2.0,0.0,...|\n",
      "|       1.0|           1.0|[35.0,1.0,25.0,1....|\n",
      "|       0.0|           0.0|[77.0,7.0,70.0,2....|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf.select(\"prediction\",\"Arrest_indexed\",\"selectedFeatures\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_25df9f49293f\n"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//GBT\n",
    "// Create a GBT model.\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"Arrest_indexed\")\n",
    "  .setFeaturesCol(\"selectedFeatures\")\n",
    "  .setMaxIter(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_25df9f49293f-maxBins: 310,\n",
       "\tgbtc_25df9f49293f-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_25df9f49293f-maxBins: 310,\n",
       "\tgbtc_25df9f49293f-maxDepth: 5\n",
       "}, {\n",
       "\tgbtc_25df9f49293f-maxBins: 315,\n",
       "\tgbtc_25df9f49293f-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_25df9f49293f-maxBins: 315,\n",
       "\tgbtc_25df9f49293f-maxDepth: 5\n",
       "}, {\n",
       "\tgbtc_25df9f49293f-maxBins: 315,\n",
       "\tgbtc_25df9f49293f-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_25df9f49293f-maxBins: 315,\n",
       "\tgbtc_25df9f49293f-maxDepth: 5\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_23463fbbec0a\n",
       "cv_gbt: org.apache.spark.ml.tuning.CrossValidator = cv_2aa6012ea963\n",
       "pipeline_gbt: org.apache.spark.ml.Pipeline = pipeline_e9c05415c026\n"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))    \n",
    "            .addGrid(gbt.maxBins, Array(310, 315, 315))\n",
    "             .build()\n",
    "//.addGrid(gbt.maxIter, Array(10, 20,100))\n",
    "//.addGrid(gbt.maxBins, Array(310, 315, 315))\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\").setLabelCol(\"Arrest_indexed\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv_gbt = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "\n",
    "val pipeline_gbt = new Pipeline().setStages(Array(cv_gbt))\n",
    "\n",
    "\n",
    "//val Array(training,testing)=housing.randomSplit(Array(0.8,0.2),111)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: features_top5_DF.type = [_c0: int, ID: int ... 46 more fields]\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_top5_DF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_gbt: org.apache.spark.ml.PipelineModel = pipeline_e9c05415c026\n"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val pipelineModel_gbt = pipeline_gbt.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|Arrest_indexed|    selectedFeatures|\n",
      "+----------+--------------+--------------------+\n",
      "|       0.0|           0.0|[77.0,7.0,70.0,3....|\n",
      "|       0.0|           0.0|[19.0,9.0,19.0,5....|\n",
      "|       1.0|           1.0|[1.0,2.0,2.0,0.0,...|\n",
      "|       1.0|           1.0|[35.0,1.0,25.0,1....|\n",
      "|       0.0|           0.0|[77.0,7.0,70.0,2....|\n",
      "|       1.0|           1.0|[4.0,0.0,4.0,7.0,...|\n",
      "|       0.0|           0.0|[2.0,0.0,3.0,0.0,...|\n",
      "|       1.0|           1.0|[9.0,2.0,8.0,3.0,...|\n",
      "|       1.0|           1.0|[163.0,2.0,157.0,...|\n",
      "|       1.0|           1.0|[21.0,2.0,22.0,1....|\n",
      "|       1.0|           1.0|[42.0,2.0,41.0,0....|\n",
      "|       1.0|           1.0|[21.0,2.0,22.0,5....|\n",
      "|       1.0|           1.0|[11.0,8.0,10.0,16...|\n",
      "|       0.0|           0.0|[8.0,3.0,5.0,6.0,...|\n",
      "|       0.0|           0.0|[12.0,10.0,11.0,1...|\n",
      "|       1.0|           1.0|[4.0,0.0,4.0,14.0...|\n",
      "|       0.0|           0.0|[41.0,0.0,40.0,40...|\n",
      "|       1.0|           1.0|[9.0,2.0,8.0,0.0,...|\n",
      "|       0.0|           0.0|(10,[0,1,6,9],[5....|\n",
      "|       0.0|           0.0|(10,[0,2,3,5,8],[...|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.821189356120801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_gbt: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n",
       "AUC_gbt: Double = 0.821189356120801\n"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val predictions_gbt = pipelineModel_gbt.transform(testing)\n",
    "predictions_gbt.select(\"prediction\",\"Arrest_indexed\",\"selectedFeatures\").show()\n",
    "\n",
    "\n",
    "val AUC_gbt = evaluator.evaluate(predictions_gbt)\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC_gbt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr.createOrReplaceTempView(\"table_lr\")\n",
    "predictions_rf.createOrReplaceTempView(\"table_rf\")\n",
    "predictions_gbt.createOrReplaceTempView(\"table_gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.mllib.evaluation._\n"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import spark.implicits._\n",
    "import org.apache.spark.mllib.evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|selectedFeatures                                |\n",
      "+------------------------------------------------+\n",
      "|[77.0,7.0,70.0,3.0,0.0,128.0,20.0,41.0,68.0,7.0]|\n",
      "|[19.0,9.0,19.0,5.0,0.0,72.0,0.0,1.0,0.0,8.0]    |\n",
      "|[1.0,2.0,2.0,0.0,0.0,193.0,15.0,7.0,34.0,1.0]   |\n",
      "|[35.0,1.0,25.0,1.0,0.0,100.0,9.0,31.0,61.0,2.0] |\n",
      "|[77.0,7.0,70.0,2.0,0.0,107.0,7.0,34.0,20.0,7.0] |\n",
      "|[4.0,0.0,4.0,7.0,0.0,37.0,4.0,5.0,12.0,0.0]     |\n",
      "|[2.0,0.0,3.0,0.0,0.0,205.0,12.0,37.0,27.0,0.0]  |\n",
      "|[9.0,2.0,8.0,3.0,0.0,14.0,0.0,1.0,8.0,1.0]      |\n",
      "|[163.0,2.0,157.0,2.0,0.0,138.0,2.0,11.0,5.0,1.0]|\n",
      "|[21.0,2.0,22.0,1.0,0.0,150.0,0.0,4.0,16.0,1.0]  |\n",
      "|[42.0,2.0,41.0,0.0,0.0,138.0,2.0,11.0,5.0,1.0]  |\n",
      "|[21.0,2.0,22.0,5.0,0.0,168.0,15.0,15.0,28.0,1.0]|\n",
      "|[11.0,8.0,10.0,16.0,0.0,27.0,2.0,10.0,9.0,3.0]  |\n",
      "|[8.0,3.0,5.0,6.0,0.0,102.0,16.0,21.0,13.0,4.0]  |\n",
      "|[12.0,10.0,11.0,15.0,0.0,34.0,4.0,13.0,15.0,9.0]|\n",
      "|[4.0,0.0,4.0,14.0,0.0,117.0,14.0,37.0,3.0,0.0]  |\n",
      "|[41.0,0.0,40.0,40.0,0.0,22.0,13.0,2.0,11.0,0.0] |\n",
      "|[9.0,2.0,8.0,0.0,0.0,115.0,7.0,1.0,2.0,1.0]     |\n",
      "|(10,[0,1,6,9],[5.0,4.0,6.0,5.0])                |\n",
      "|(10,[0,2,3,5,8],[7.0,7.0,10.0,150.0,16.0])      |\n",
      "+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select selectedFeatures from table_lr\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joins: org.apache.spark.sql.DataFrame = [Arrest_indexed: double, prediction_lr: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joins = spark.sql(\"select l.Arrest_indexed,l.prediction as prediction_lr,r.prediction as prediction_rf,g.prediction as prediction_gbt from table_lr l,table_rf r,table_gbt g where l.selectedFeatures = r.selectedFeatures and l.selectedFeatures = g.selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ensemble: org.apache.spark.sql.DataFrame = [prediction_ensemble: double, Arrest: double]\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joins.createOrReplaceTempView(\"join_temp\")\n",
    "val ensemble = spark.sql(\"select CASE WHEN (prediction_lr = prediction_rf OR prediction_lr = prediction_gbt) Then prediction_lr else case when prediction_rf=prediction_gbt then prediction_rf else prediction_lr END  END AS prediction_ensemble,Arrest_indexed as Arrest from  join_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[18155] at map at <console>:178\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsAndLabels=ensemble.selectExpr(\"cast(prediction_ensemble as Double) prediction_ensemble\", \"cast(Arrest as Double) Arrest\").rdd.map(row =>(row.getAs[Double](\"prediction_ensemble\"),row.getAs[Double](\"Arrest\")))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4ea7bb3e\n",
       "AUC_EN: Double = 0.8089813196530709\n"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new BinaryClassificationMetrics(predictionsAndLabels)\n",
    "val AUC_EN = metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve(AUC) for LR on test data = 0.5958459707451914\n",
      "Area under ROC curve(AUC) for RF on test data = 0.794775709187095\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.821189356120801\n",
      "Area under ROC curve(AUC) for Ensemble on test data = 0.8089813196530709\n"
     ]
    }
   ],
   "source": [
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC_lr\")\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC_rf\")\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC_gbt\")\n",
    "println(s\"Area under ROC curve(AUC) for Ensemble on test data = $AUC_EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg._\n",
       "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
       "import org.apache.spark.ml.classification.RandomForestClassifier\n"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res57: org.apache.spark.sql.DataFrame = [_c0: int, ID: int ... 49 more fields]\n"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureImportanceGBT: org.apache.spark.ml.linalg.Vector = (10,[0,3,5],[0.7832404957880407,0.1563977306224484,0.06036177358951093])\n"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val featureImportanceGBT = pipelineModel_gbt.stages(0).asInstanceOf[CrossValidatorModel].bestModel.asInstanceOf[GBTClassificationModel].featureImportances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureImportanceRF: org.apache.spark.ml.linalg.Vector = (10,[0,1,2,3,4,5,6,7,8,9],[0.201137613975929,0.30893347257301795,0.21078700596956676,0.012787054916409564,0.010964876735941242,0.023033123943009555,0.0032388114583067535,0.001102700050240262,0.0054026122436715805,0.2226127281339072])\n"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureImportanceRF = pipelineModel_rf.stages(0).asInstanceOf[CrossValidatorModel].bestModel.asInstanceOf[RandomForestClassificationModel].featureImportances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeric_features_array: Array[Any] = Array(0.0, 1.0, 1.0, (41.782921527, -87.60436317), 1.0, 112.0, 8.0, 7.0, 24.0, 2.0, 12.0, 10.0, 11.0, (41.88063228, -87.635935494), 0.0, 57.0, 13.0, 4.0, 11.0, 9.0, 7.0, 0.0, 7.0, (41.771073064, -87.568278663), 0.0, 45.0, 8.0, 18.0, 4.0, 0.0, 12.0, 10.0, 11.0, (41.99973106, -87.705809711), 0.0, 172.0, 19.0, 43.0, 31.0, 9.0, 16.0, 6.0, 15.0, (41.839947022, -87.714672499), 0.0, 211.0, 7.0, 34.0, 20.0, 6.0, 2.0, 0.0, 3.0, (41.885753285, -87.626996239), 0.0, 22.0, 13.0, 2.0, 11.0, 0.0, 77.0, 7.0, 70.0, (41.980259177, -87.710009782), 0.0, 128.0, 20.0, 41.0, 68.0, 7.0, 11.0, 8.0, 10.0, (41.909508082, -87.755180702), 0.0, 8.0, 3.0, 9.0, 0.0, 3.0, 11.0, 8.0, 10.0, (41.76971848, -87.584270226), 0.0, 78.0, 8.0, 18.0, 4.0, 3.0, 19.0, 9.0, 19.0, (41.875039579, -..."
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numeric_features_array = numeric_features.collect.map(_.toSeq).flatten\n",
    "\n",
    "//val features_gbt= numeric_features++Array(\"selectedFeatures\")\n",
    "//featureImportanceGBT.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features_gbt: Array[Any] = Array(0.0, 1.0, 1.0, (41.782921527, -87.60436317), 1.0, 112.0, 8.0, 7.0, 24.0, 2.0, 12.0, 10.0, 11.0, (41.88063228, -87.635935494), 0.0, 57.0, 13.0, 4.0, 11.0, 9.0, 7.0, 0.0, 7.0, (41.771073064, -87.568278663), 0.0, 45.0, 8.0, 18.0, 4.0, 0.0, 12.0, 10.0, 11.0, (41.99973106, -87.705809711), 0.0, 172.0, 19.0, 43.0, 31.0, 9.0, 16.0, 6.0, 15.0, (41.839947022, -87.714672499), 0.0, 211.0, 7.0, 34.0, 20.0, 6.0, 2.0, 0.0, 3.0, (41.885753285, -87.626996239), 0.0, 22.0, 13.0, 2.0, 11.0, 0.0, 77.0, 7.0, 70.0, (41.980259177, -87.710009782), 0.0, 128.0, 20.0, 41.0, 68.0, 7.0, 11.0, 8.0, 10.0, (41.909508082, -87.755180702), 0.0, 8.0, 3.0, 9.0, 0.0, 3.0, 11.0, 8.0, 10.0, (41.76971848, -87.584270226), 0.0, 78.0, 8.0, 18.0, 4.0, 3.0, 19.0, 9.0, 19.0, (41.875039579, -87.7436902..."
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val features_gbt= numeric_features_array++Array(\"selectedFeaturess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features_rf: Array[Any] = Array(0.0, 1.0, 1.0, (41.782921527, -87.60436317), 1.0, 112.0, 8.0, 7.0, 24.0, 2.0, 12.0, 10.0, 11.0, (41.88063228, -87.635935494), 0.0, 57.0, 13.0, 4.0, 11.0, 9.0, 7.0, 0.0, 7.0, (41.771073064, -87.568278663), 0.0, 45.0, 8.0, 18.0, 4.0, 0.0, 12.0, 10.0, 11.0, (41.99973106, -87.705809711), 0.0, 172.0, 19.0, 43.0, 31.0, 9.0, 16.0, 6.0, 15.0, (41.839947022, -87.714672499), 0.0, 211.0, 7.0, 34.0, 20.0, 6.0, 2.0, 0.0, 3.0, (41.885753285, -87.626996239), 0.0, 22.0, 13.0, 2.0, 11.0, 0.0, 77.0, 7.0, 70.0, (41.980259177, -87.710009782), 0.0, 128.0, 20.0, 41.0, 68.0, 7.0, 11.0, 8.0, 10.0, (41.909508082, -87.755180702), 0.0, 8.0, 3.0, 9.0, 0.0, 3.0, 11.0, 8.0, 10.0, (41.76971848, -87.584270226), 0.0, 78.0, 8.0, 18.0, 4.0, 3.0, 19.0, 9.0, 19.0, (41.875039579, -87.74369026..."
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val features_rf= numeric_features_array++Array(\"selectedFeaturess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,0.7832404957880407)\n",
      "(1.0,0.0)\n",
      "(1.0,0.0)\n",
      "((41.782921527, -87.60436317),0.1563977306224484)\n",
      "(1.0,0.0)\n",
      "(112.0,0.06036177358951093)\n",
      "(8.0,0.0)\n",
      "(7.0,0.0)\n",
      "(24.0,0.0)\n",
      "(2.0,0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res_gbt: Unit = ()\n"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res_gbt = features_gbt.zip(featureImportanceGBT.toArray).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,0.201137613975929)\n",
      "(1.0,0.30893347257301795)\n",
      "(1.0,0.21078700596956676)\n",
      "((41.782921527, -87.60436317),0.012787054916409564)\n",
      "(1.0,0.010964876735941242)\n",
      "(112.0,0.023033123943009555)\n",
      "(8.0,0.0032388114583067535)\n",
      "(7.0,0.001102700050240262)\n",
      "(24.0,0.0054026122436715805)\n",
      "(2.0,0.2226127281339072)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res_rf: Unit = ()\n"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res_rf = features_rf.zip(featureImportanceRF.toArray).foreach(println)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
