{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Example in Spark\n",
    "This example demonstrates running a simple linear regression as well as a Gradient Boosted Tree regression model in spark. We will use California housing dataset from this kaggle competition: https://www.kaggle.com/camnugent/california-housing-prices/version/1 You can download the dataset from here: https://uofi.box.com/s/ibp6witxuq5udtvalex7s4h8selwfol5. This dataset consists of 9 predictive variables on about 20.6K observations. The dataset is not big; however, the program we will have here is completely scalable and can be run on big data.  Our goal is to build a regression model which can predict the median_house value per block group based on the features collected in this dataset If you look at the data description on kaggle, it states that all variable are per block group. \"A Census Block Group is a geographical unit used by the United States Census Bureau which is between the Census Tract and the Census Block. It is the smallest geographical unit for which the bureau publishes sample data, i.e. data which is only collected from a fraction of all households\"(wikipedia).\n",
    "\n",
    "The goal of this notebook is to learn how to build and train Linear Regression and Gradient Boosted Regression Tree models in spark, tune their hyper-parameters and evaluate them using cross-validation.\n",
    "\n",
    "As before, let's first configure our spark shell on yarn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='2600m'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Exploring Data\n",
    "I have copied the data to hdfs, let's load the data in spark, see the schema, and print a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 11:56:22 WARN  CacheManager:66 - Asked to cache already cached data.\n",
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "housing_df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n",
       "res2: Long = 20640\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Read the CSV file and load it into a dataframe. Note that the \"inferschema\" parameter is set to true\n",
    "val housing_df=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").csv(\"/hadoop-user/data/housing.csv\")\n",
    "housing_df.cache()\n",
    "housing_df.printSchema()\n",
    "housing_df.show(3)\n",
    "housing_df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all the variables are contineous except for ocean_proximity. Let's get a quick distirbution of this categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|ocean_proximity|count(ocean_proximity)|\n",
      "+---------------+----------------------+\n",
      "|         ISLAND|                     5|\n",
      "|     NEAR OCEAN|                  2658|\n",
      "|       NEAR BAY|                  2290|\n",
      "|      <1H OCEAN|                  9136|\n",
      "|         INLAND|                  6551|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_df.createOrReplaceTempView(\"housing\")\n",
    "spark.sql(\"select ocean_proximity,count(ocean_proximity) from housing group by ocean_proximity\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some exploratory data analysis. First let's check some statistics on each column (number of rows,min, max, standard deviation,etc.). In spark, you can use the describe method of the dataframe to get a basic summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|summary|          longitude|         latitude|housing_median_age|       total_rooms|    total_bedrooms|        population|       households|     median_income|median_house_value|ocean_proximity|\n",
      "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|  count|              20640|            20640|             20640|             20640|             20433|             20640|            20640|             20640|             20640|          20640|\n",
      "|   mean|-119.56970445736148| 35.6318614341087|28.639486434108527|2635.7630813953488| 537.8705525375618|1425.4767441860465|499.5396802325581|3.8706710029070246|206855.81690891474|           null|\n",
      "| stddev|  2.003531723502584|2.135952397457101| 12.58555761211163|2181.6152515827944|421.38507007403115|  1132.46212176534|382.3297528316098| 1.899821717945263|115395.61587441359|           null|\n",
      "|    min|            -124.35|            32.54|               1.0|               2.0|               1.0|               3.0|              1.0|            0.4999|           14999.0|      <1H OCEAN|\n",
      "|    max|            -114.31|            41.95|              52.0|           39320.0|            6445.0|           35682.0|           6082.0|           15.0001|          500001.0|     NEAR OCEAN|\n",
      "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stat: org.apache.spark.sql.DataFrame = [summary: string, longitude: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stat=housing_df.describe()\n",
    "stat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see because of the number of features it is hard to see the statistics on each feature. It is better to transpose this dataset, that is to flip the rows and columns, so we can have features as rows and their stattistics as columns. Unfortunately, spark does not have a built-in feature for transposing a dataframe. Spylon allows us to share spark dataframes between python . You just need to create a temporary view from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat.createOrReplaceTempView(\"stat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then wee can use %%python to switch to pyspark. Since the dataframe is small $5\\times10$ (five descriptive values for 10 columns) we can convert it to a non-distributed python dataframe using spark toPandas method. This method acts similar to collect in that it collects the entire dataset to the driver, except that it collects data as a python dataframe which resides in memory of the driver node. We can use the transpose method now from \"pandas\" library in python to transpose the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0                    1                   2          3           4\n",
      "summary             count                 mean              stddev        min         max\n",
      "longitude           20640  -119.56970445736148   2.003531723502584    -124.35     -114.31\n",
      "latitude            20640     35.6318614341087   2.135952397457101      32.54       41.95\n",
      "housing_median_age  20640   28.639486434108527   12.58555761211163        1.0        52.0\n",
      "total_rooms         20640   2635.7630813953488  2181.6152515827944        2.0     39320.0\n",
      "total_bedrooms      20433    537.8705525375618  421.38507007403115        1.0      6445.0\n",
      "population          20640   1425.4767441860465    1132.46212176534        3.0     35682.0\n",
      "households          20640    499.5396802325581   382.3297528316098        1.0      6082.0\n",
      "median_income       20640   3.8706710029070246   1.899821717945263     0.4999     15.0001\n",
      "median_house_value  20640   206855.81690891474  115395.61587441359    14999.0    500001.0\n",
      "ocean_proximity     20640                 None                None  <1H OCEAN  NEAR OCEAN\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import pandas as pd\n",
    "stat_python=spark.sql(\"select * from stat\" )\n",
    "stat_python_nonDistributed=stat_python.toPandas().transpose()\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(stat_python_nonDistributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"count\" column above shows the number of non-null entries for each feature. It looks like \"total_bedrooms\" feature has some null entries or missing values.  Two general cataegories of methods to deal with missing values are: 1- complete case analysis,and 2- data imputation. In complete case analysis we simply get rid of all the rows with missing values in any of the columns. This method can be used when your dataset is not too small and when only a small percentage of your rows have missing values. If there are a large number of rows with missing values, then throwing away all of those rows can result in loss of information and possibly a weak machine learning model. In that case, you should use data imputation to infer the missing value from the rest of the data. There are a variety of imputation methods, the easiest one being replacing all the missing values with the mean of the colum.However, the mean imputation suffers from a number of shortcomings. For a more detailed discussion on missing value imputation, please refer to this tutorial: http://www.stat.columbia.edu/~gelman/arm/missing.pdf. In this example, however only about 200 rows out of 20620 have missing values and that is less than 1% of the data so it is reasonable to drop the rows with  missing values. we can use the funciton na.drop() in spark to get rid of all the rows with null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing_complete: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n",
       "res6: Long = 20433\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housing_complete=housing_df.na.drop()\n",
    "housing_complete.createOrReplaceTempView(\"housing\")\n",
    "housing_complete.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Engineering\n",
    "Now let's do some feature engineering. Feature engineering is typically referred to as creating new features from existing features. For this particular example we need to do two things:\n",
    "\n",
    "1. If you look at the summary statistics above, you will see that variables are on different scale and the range between min and max values greatly varies between features. So that tells us that we need to scale our data. Let's scale the numeric predictors using StandardScaler. Please note that scaling of the target/outcome variable (median_house_value) is not neccessary. Nevertheless, since the median_house_values are quite large it will make the interpretation easier if we divide each value by 100K, that way instead of the unit of house_value being a dollar it will be 100K dollars and a median house value of 5 for example will represent a $500K value. \n",
    "\n",
    "\n",
    "2. The categorical predictor \"ocean_proximity\" needs to be converted to a numeric value before we can feed it to a machine learning algorithm, so we can use StringIndexer to convert this categorical variable to category indices. Since this variable does not have a natural ordering, we should also use one-hot-encoding on top of stringIndexer.\n",
    "\n",
    "Before feeding a dataset to a machine learning algorithm in spark, we need to convert it into (features,label) form where features is a numeric vector of predictors and label is a numeric target variable. \n",
    "\n",
    "In code segment below we first create a numeric vector from all numeric features and standardize this vector using standardScaler. Then we conver the categorical variable \"ocean_proximity\" to its one-hot-encoding and assemble that with our previously created numeric feature vector. This gives us a numeric vecotr with all the predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n",
       "import org.apache.spark.ml.feature._\n",
       "numeric_features: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income)\n",
       "vectorizer_numeric: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_bdbe02e86e0e\n",
       "standardizer: org.apache.spark.ml.feature.StandardScaler = stdScal_fd0d52f6f28c\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_91cc971014eb\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_c79bf424bfc2\n",
       "vectorizer_all: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_cb96cfb0c251\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* the \"withColumn\" method of data frame can be used to replace the values of a column or create new column based on an existing column\n",
    " * It takes two parameters: 1- the name of the column, 2- the column values\n",
    " * We use withColumn to divide the target variable median_house_value by 100,000*/\n",
    "val housing=housing_complete.withColumn(\"median_house_value\", housing_complete(\"median_house_value\")/100000)\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "//get all the numeric features except the target variable\n",
    "val numeric_features=housing_complete.columns.filter(c => !c.equals(\"ocean_proximity\") && !c.equals(\"median_house_value\"))\n",
    "\n",
    "//Use VectorAssesmbler to aseemble numeric features into a vector\n",
    "val vectorizer_numeric=new VectorAssembler().setInputCols(numeric_features).setOutputCol(\"numeric_features\")\n",
    "\n",
    "//Create an estimator to standardize the numeric feature\n",
    "val standardizer=new StandardScaler().setWithMean(true).setInputCol(\"numeric_features\").setOutputCol(\"numeric_features_vector\")\n",
    "\n",
    "//Do one-hot-encoding of the \"ocean_proximity\" variable\n",
    "val indexer=new StringIndexer().setInputCol(\"ocean_proximity\").setOutputCol(\"ocean_proximity_indexer\")\n",
    "val encoder= new OneHotEncoderEstimator().setInputCols(Array(\"ocean_proximity_indexer\")).setOutputCols(Array(\"ocean_proximity_coded\"))\n",
    "\n",
    "//Now let's add the encoded cataegorical variable to our feature vector using VectorAssembler again\n",
    "val vectorizer_all=new VectorAssembler().setInputCols(Array(\"numeric_features_vector\",\"ocean_proximity_coded\")).setOutputCol(\"features\")\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the linear regression model for prediction\n",
    "Now that we preprocessed our data,  we are ready to create a linear regression model and fit it to the training data. You can use \"LinearRegression\" to create a linear regression model in spark. The \"setLabelCol\" method specifies the name of the target variable and the \"setFeaturesCol\" method specifies the name of the vector of predictors. \"setMAxIter\" sets the maximum number of iterations used in optimizing the cost function for linear regression, \"setRegParam\", and \"setElasticNetParam\" sepecify the values for lambda and alpha in elastic net regularization, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.regression._\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_06a106ba8921\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.regression._\n",
    "//Creating the linearRegression model and fit it to the transformed training data\n",
    "val lr= new LinearRegression().setLabelCol(\"median_house_value\").setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Pipeline\n",
    "If you notice, we have not yet fit any of the preprocesisng steps on our data, neither did we fit the regression model to our data. Instead of fitting and transforming data separately in each step of feature engineering and machine learning model, we can create a Pipeline object and add all the preprocessing and regression stages we did so far to this pipeline. A single  call of \"fit\" or \"transform\" on the Pipeline object will put data through all the fitting or transformations in the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_7d090ae608a3\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creating a Pipeline and add the transformation we did so far to this pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(vectorizer_numeric,standardizer,indexer,encoder,vectorizer_all, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the pipeline, we van use the method randomSplit on dataframe to split the data randomly to train and test set . randomSplit takes an array of training and testing proportions ( we use 80% for training and 20% for testing) and a seed for random number generator (same seeds produce the same random split) .\n",
    "Then we fit the pipeline to the training data and transform the training and testing data. This will apply all the preprocessing steps to our training data and build a regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [longitude: double, latitude: double ... 8 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [longitude: double, latitude: double ... 8 more fields]\n",
       "pipeline_model: org.apache.spark.ml.PipelineModel = pipeline_7d090ae608a3\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training,testing)=housing.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fitting the pipeline to the traning data and transforming the training data\n",
    "val pipeline_model= pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the regression model\n",
    "Once we built the pipeline model on the training data, We can apply it to the test data to compute the predicted median_house_value for each predictors vector in the testing data. Please note that the pipeline should NOT be fit on the testing data. The testing data should only be reserved for testing and Not used for fitting the model. You should only call transform on the testing data to apply the model that is alreaday built on the training data.\n",
    "\n",
    "Once we have the predictions on the test data, we can use Root Mean Squared Error (RMSE) evaluation metric to see how these predictions deviate from the actual median_house_values in the test data. The transform method on the pipeline_model will create a new dataframe with an additional \"predictions\" column. We can then use spark's \"RegresionEvaluator\" to evaluate our regression model. We should specify the target(i.e., label) column and prediction column as well as the metric we want to use to evaluate our regression model (we used rmse here).\n",
    "\n",
    "The rmse of our linear regression model is about 0.82. Intuitively this means that on average our predicted house values deviates about 82000 dolalrs from the actual house values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|        prediction|median_house_value|            features|\n",
      "+------------------+------------------+--------------------+\n",
      "|1.7148592037835118|             0.761|[-2.3332227378130...|\n",
      "|1.7319496212199748|             0.669|[-2.3132569008644...|\n",
      "|2.1296595027817435|             0.901|[-2.3032739823901...|\n",
      "|1.8450013018575213|              0.69|[-2.3032739823901...|\n",
      "|1.5913243599391245|             0.646|[-2.2982825231530...|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.8211974817480996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "predictions: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 14 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_b02fd9b71da7\n",
       "rmse: Double = 0.8211974817480996\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions = pipeline_model.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"prediction\", \"median_house_value\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"median_house_value\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and HyperParameter Tunning\n",
    "There were a few hyper-parameter in our linear regression model that we arbitrarily set: max iterations, regularization parameter (lambda), and elastic net parameter(alpha)).  Now we want to try a range of values for this parameter to see if we can get a better model. In other words, we would like to tune the models' hyper-parameter to get a better prediction. Let's use param-grid together with cross validation to select a combination parameter which gives the best RMSE. We can build a hyperparameter grid in spark by using \"ParamGridBuilder\" and call addGrid to add a set of values for a hyperparameter to the grid.\n",
    "Here we try three arbitrary values for lambda (regParam), three arbitrary values for alpha(elasticNetParam) and two values for max number of iterations for solving the linear regression. That gives us a $3\\times 3\\times 2=18$ different combinations to try for the parameters. \n",
    "\n",
    "After building a parameter grid, we can use \"CrossValidator\" in spark to run a cross-validation on the paramGrid to evaluate the regression model built based on each combination. We have to provide the model, the parameter grid and the evaluator to our CrossValidator. I also set the number of folds to 10. This will split the train data to 10 parts and then train the model 10 times, with a separate part used for validation each time. This is done for each parameter combination in the param grid. So altogether, we are fitting a total of 10*18=180 linear regression models.\n",
    "\n",
    "We create a new pipeline which includes all the stages of preprocessing plus the cross validation stage. Note that the cross validation stage, already includes the linear regression model so there is no need to add lr as a separate stage to the new pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning._\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinReg_06a106ba8921-elasticNetParam: 0.0,\n",
       "\tlinReg_06a106ba8921-maxIter: 5,\n",
       "\tlinReg_06a106ba8921-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_06a106ba8921-elasticNetParam: 0.0,\n",
       "\tlinReg_06a106ba8921-maxIter: 10,\n",
       "\tlinReg_06a106ba8921-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_06a106ba8921-elasticNetParam: 0.5,\n",
       "\tlinReg_06a106ba8921-maxIter: 5,\n",
       "\tlinReg_06a106ba8921-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_06a106ba8921-elasticNetParam: 0.5,\n",
       "\tlinReg_06a106ba8921-maxIter: 10,\n",
       "\tlinReg_06a106ba8921-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_06a106ba8921-elasticNetParam: 1.0,\n",
       "\tlinReg_06a106ba8921-maxIter: 5,\n",
       "\tlinReg_06a106ba8921-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_06a106ba8921-elasticNetParam: 1.0,\n",
       "\tlinReg_06a106ba8921-maxIter: 10,\n",
       "\tlinReg_06a106ba8921-regParam: 0.0..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.ml.tuning._\n",
    "\n",
    "//Create ParamGrid for Cross Validation to the linear regression model\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .addGrid(lr.maxIter, Array(5, 10))\n",
    "             .build()\n",
    "// Create 10-fold CrossValidator\n",
    "val cv = new CrossValidator().setEstimator(lr).setEstimatorParamMaps(paramGrid).setEvaluator(evaluator).setNumFolds(10)\n",
    "\n",
    "//uUPDATING THE PIPE\n",
    "val new_pipeline=new Pipeline().setStages(Array(vectorizer_numeric,standardizer,indexer,encoder,vectorizer_all, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit this new_pipeline to our training data to build a new model and apply it to the testing data. The best model is then used to generate predictions on the test data. We see below that by tuning hyper-parameter we were able to achieve about 14% improvement in our RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) of the best model on the test data = $rmse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "new_Model: org.apache.spark.ml.PipelineModel = pipeline_3c8d809002f1\n",
       "predictions: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 14 more fields]\n",
       "rmse: Double = 0.7027095891602264\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "// Fit the new pipeline to the training data.This will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "val new_Model = new_pipeline.fit(training)\n",
    "\n",
    "//new_Model uses the best model found from the Cross Validation.We Use test set to measure the accuracy of our model on new data\n",
    "val predictions = new_Model.transform(testing)\n",
    "val rmse=evaluator.evaluate(predictions)\n",
    "println(\"Root Mean Squared Error (RMSE) of the best model on the test data = $rmse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Gradient Boosted Tree Regression Model\n",
    "Now let's try to solve this problem using Gradient Boosted Tree (GBT) models. For GBT models, you don't need to scale your numeric feature or do one-hot-encoding of categorical feature. All we have to do is to convert the String features to indicecs using StringIndexer and assemble all the features ( except the target variable) as a vector. We can then create a GBT regression model usin GBTRegressor and evaluate it using cross validation. The two hyper-parameter we tune here using the ParamGrid are 1-maxDepth ( the maximum depth of each decision tree, this controls the overfitting due to complexity of each tree) and 2- maxIteration ( The maximum number of trees in the ensemble). \n",
    "\n",
    "We create a pipeline of StringIndexer, VectorAssembler, and crossValidation stages, fit our pipeline model to training data and use it to transfer and predict the median_house_value for test data. Finally, we evaluate the predictions on test data using RMSE.\n",
    "\n",
    "When you run this, be patient, We are building a lot of trees. Precisely, we are building $(100+20+10)\\times 2=260$ decision tree models. So get a coffee or a beer and relax for a while. It will take some time to run on our tiny three node cluster. You might get a couple \"warnings\", just ignore them and let the code run to completion. \n",
    "After the code completes, you can see that the RMSE on the test data is decreased by almost 27% using the GBT model. This is probably due to the fact that the GBT model was able to capture some nonlinear relationship between the outcome and the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val housing=housing_complete.withColumn(\"median_house_value\", housing_complete(\"median_house_value\")/100000)\n",
    "\n",
    "\n",
    "//get all the numeric features except the target variable\n",
    "val numeric_features=housing.columns.filter(c => !c.equals(\"ocean_proximity\") && !c.equals(\"median_house_value\"))\n",
    "\n",
    "\n",
    "//index the \"ocean_proximity\" variable\n",
    "val indexer=new StringIndexer().setInputCol(\"ocean_proximity\").setOutputCol(\"ocean_proximity_indexer\")\n",
    "\n",
    "//Now let's assemble everyting together in a feature vector\n",
    "val vectorizer=new VectorAssembler().setInputCols(numeric_features++Array(\"ocean_proximity_indexer\")).setOutputCol(\"features\")\n",
    "\n",
    "// Create a GBT model.\n",
    "val gbt = new GBTRegressor()\n",
    "  .setLabelCol(\"median_house_value\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "\n",
    "\n",
    "//Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))\n",
    "             .addGrid(gbt.maxIter, Array(10, 20,100))\n",
    "             .build()\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"median_house_value\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "\n",
    "val cv = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)\n",
    "\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(indexer, vectorizer,cv))\n",
    "\n",
    "\n",
    "val Array(training,testing)=housing.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"prediction\", \"median_house_value\", \"features\").show(5)\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the feature Importance\n",
    "Once we built the model we can get the variable importance of the best model (the model with the least MSE on the cross validation set). The variable importance is a numeric vector which gives each feature a number between [0,1] indicating the importance of that feature in predicting the outcome.  \n",
    "\n",
    "To get the variable importance we first have to access the best model. pipelineModel.stages gives us the array of stages of the pipelineModel. The cross validation stage (cv) is the third stage in our pipelineModel and we can access it by index 2, cast it to CrossValidatorModel and get its bestModel. Then we cast the best Model to GBTRegressionModel and use featureImportances method to get the variable importance. This will give us a numeric vector of variable importance. Then we zip this vector to the feature names vector which we used to build our model and sort it in the descending order of its importance.\n",
    "\n",
    "The result shows that the location (latitude and longtitude), median_household_income, and the median_age of the house were the most important features in predicting the median house value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(longitude,0.19152242888428286)\n",
      "(latitude,0.1622161601481415)\n",
      "(median_income,0.15253168850003201)\n",
      "(housing_median_age,0.11890735656062079)\n",
      "(population,0.09867196284202912)\n",
      "(ocean_proximity_indexer,0.07987662505311782)\n",
      "(total_rooms,0.0782894711037661)\n",
      "(total_bedrooms,0.06015861437288235)\n",
      "(households,0.05782569253512725)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg._\n",
       "featureImportance: org.apache.spark.ml.linalg.Vector = (9,[0,1,2,3,4,5,6,7,8],[0.19152242888428286,0.1622161601481415,0.11890735656062079,0.0782894711037661,0.06015861437288235,0.09867196284202912,0.05782569253512725,0.15253168850003201,0.07987662505311782])\n",
       "features: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, ocean_proximity_indexer)\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//print variable importance.\n",
    "import org.apache.spark.mllib.linalg._\n",
    "val featureImportance=pipelineModel.stages(2).asInstanceOf[CrossValidatorModel].bestModel.asInstanceOf[GBTRegressionModel].featureImportances\n",
    "\n",
    "val features= numeric_features++Array(\"ocean_proximity_indexer\")\n",
    "val res = features.zip(featureImportance.toArray).sortBy(-_._2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
